\documentclass[thesis.tex]{subfiles}
\begin{document}

\chapter{Discussion}

In this chapter we will discuss the optimal results of the work described in our report.
First we compare our descriptors and results with SIFT and HOG individually. Secondly we discuss the differences in descriptors and their optimal parameters between the two applications.
Finally we discuss possible future work.

\section{Image correspondence}
\label{sec:discussionIc}
Recall that the image correspondence evaluation shows that our descriptors are very close performance-wise, and marginally outperform SIFT. The difference is larger for light variations and statistically significant in a few cases. With these results in mind, we will now compare and reflect on the different design choices of SIFT and our descriptor.

\todo{Reason about the normal distribution assumption in the statistics}

Where SIFT uses vector normalization and clipping for illumination invariance and robustness, we use pixel-wise normalization and normalization of the final vector. The choice of using local illumination estimation and normalization is most likely the reason for the difference in performance under light variations.

Another difference between SIFT and our descriptor is that we don't support rotation estimation. The DTU dataset images have fixed orientation and hence we have omitted this step from SIFT too in order to avoid a decreased performance due to wrong rotation estimation. We have verified that including this step indeed reduces performance. The rotation estimation of SIFT could easily be adopted by our descriptor as an initial step for rotation invariance. Our cell layouts would then be rotated according to the estimated orientation.

The cell layouts of SIFT and our descriptors differ greatly. Our descriptors are based on polar grids such as GLOH and DAISY, whereas SIFT uses a square grid. Assuming that structure close to the interest point is more important than structure further away, we see two advantages of our grids. The corner cells of SIFT are further away than the other edge cells, while our cells in each ring have equal distance to the interest point. The SIFT grid contains more outer-cells than inner-cells, while our rings have have an equal amount of cells. This gives an even distribution of cells in both angular and radial direction. According to \citet{cui2009scale} polar grids are more robust against scale estimation errors and hence this should apply to our descriptors as well. Furthermore we believe that our grids are be more robust against rotation estimation errors, since the cell areas would overlap more with the correctly rotated grid than for a squared grid. This will however need to be tested before coming to a conclusion.

We see that we have more cells and bins than SIFT resulting in a descriptor with larger dimensionality than SIFT. This is related to the fact that our descriptor cells are spread across a larger area, and thus the amount of samples in each cell histogram is closer to that of SIFT than the dimensionality suggests.

Our initial idea was to base our cell histograms on locally orderless images \cite{koenderink1999structure}. This work is based on Gaussian kernels for bin and cell aperture functions rather than the triangle kernels of SIFT. Our parameter study has shown that the two kernels produce almost identical results, but the triangle kernels are less expensive to compute in practice. This is due to the complexity of the Gaussian kernels and their infinite support, which we limit to 3$\beta$, while the simple triangle kernels only have $2\beta$ support (see \Cref{sec:apertureKernelFunctions}).

The center aperture function, which SIFT likewise uses, only has a minor influence on the performance of our descriptor. Our performance gain from this function is most likely not as high as for SIFT, since our cell layouts already favour the structure closer to the interest point.

From our test results we see that the addition of shape index to GO only increases performance marginally. The addition however adds a substantial number of dimensions to the descriptor.
Given the tiny performance increase we recommend using the GO descriptor on its own.

\section{Pedestrian detection}
\label{sec:discussionOd}

Our pedestrian detection evaluation shows that our descriptors are not as good as HOG, with our compact GO+SI descriptor right behind. SI does not work well on its own, but adding it to both GO and HOG increases performance.

A big difference between GO and HOG is in the strategy of local normalization. An advantage of HOG's block normalization is that each cell contains four different normalization factors for different directions away from the cell. This information is different from the histogram bins, because it is based on relative magnitudes rather than orientations. An advantage of our pixel-wise normalization is that the normalization region is defined per pixel instead of per block. This allows for pixels inside a cell to be normalized individually, where HOG normalizes them equally. Interestingly, our optimal normalization region defined by a Gaussian window with $\eta = 4$ is similar in size to the four overlapping $8 \times 8$ pixel block normalizations from HOG. Another difference between GO and HOG is that HOG computes both undirected and directed gradients.

Looking at \Cref{tbl:INRIAparams}, we both have a larger density and cell overlap compared to HOG. This results in a large number of cells that are slightly larger than HOG's. Assuming a fixed number of cells, increasing the overlap between cells is a trade-off between two factors: specific gradient information will be spread out to multiple cells, increasing the robustness to small changes in pedestrian postures. However, this also reduces the influence of the information to each cell. This causes the classification ability of easily recognisable pedestrians to decrease, but a larger range of harder pedestrians to score well. The same reasoning can be said for overlap between histogram bins.

Recall that compact GO+SI is constructed from picking slightly worse parameters than the optimal in order to achieve a lower dimensionality than HOG. The initial PR AUCs shown in \Cref{tbl:inriaResults} indicate that GO+SI is significantly better than compact GO+SI. Retraining on additional hard negatives removes this performance gap and compact GO+SI takes a slight lead. Taking the non-optimal parameters into account this is surprising.....
.....

	- Initial PR AUC is not necessarily representative for the performance after hard training

HOG+SI

.....
HOG is still superior to compact GO+SI, but if we should recommend one of our descriptors, compact GO+SI would be the one.

\section{Differences between applications}
When starting the project, we had the idea, that we could create a descriptor which generalized well to different applications. However as seen in \Cref{sec:icParameterStudy,sec:odParameterStudy} we need both different design choices and different parameters for the image correspondence and pedestrian detection applications.

First of all we had to construct vastly different cell layouts depending on the use of an interest point detector or a sliding window. Secondly both smoothing of scale-space images and normalization of the final descriptor adds a minor improvement to image correspondence, while greatly decreasing performance for pedestrian detection. Finally most of the optimal parameters for pedestrian detection differ significantly from image correspondence: For example the normalization scale and the overlap between cells is much larger.

Some of the differences can be explained by the difference in the nature of the problems. In image correspondence we try to create distinctive descriptors for each individual feature, whereas in pedestrian detection we try to generalize the class of pedestrians. For example colour information is not a good indicator in pedestrian detection, but it has proven useful for finding image correspondences.

\section{Future work} % rename to alternative approaches?
%
Our pixel-wise normalization scheme, while significantly improving results over no normalization, has the downside of boosting noise in dark areas. An example of this can be seen in \Cref{fig:pixelNormalizationExample}. It may be possible to create a clever scheme to keep the increased robustness to local illumination changes while removing the noise from dark areas.

We decided to resize scale-space images respective to their scales, such that histograms are computed across the same pixel area regardless of detection scale. This means that potentially a lot of information is lost from downsampling the large scale images. It is possible to omit the resizing step and compute histograms at their original scales, but while experimenting with this we quickly ran into time and memory problems. The problem is even medium size features at $\sigma = 8$ multiply the computation time and memory by $8^2 = 64$. Another problem is whether the bin scales should be compensated for changing the number of pixels in histograms. While the approach was infeasible for us to use, there may be ways to overcome these issues.

A possible issue with constructing our shape index histograms is the non-uniformity of the distribution of shape indices in natural images, which is reported in \citep[Fig. 3]{lillholm2009statistics}. This figure shows large peaks at what is equivalent to $S = \pm 1/2$ and very few shape indices around $S = \pm 1$, which is consistent with our own findings, e.g. \Cref{fig:cellHistFigureSiCfeature}. The result of this is that some of the histogram bins have a very little effect on our distance measure. One could investigate ways of adjusting the histograms to be more uniform, possibly by moving the bin positions and scales and/or weighting them differently.

As mentioned in \Cref{sec:icParameterStudy}, our scheme for optimizing parameters risks getting stuck in local optima. We defined our parameters to be as uncorrelated as possible, but it could still be the case that modifying several of the parameters together would give a better result. The initialization of the parameters could likewise give a suboptimal solution. A possible way to improve our parameter optimization, while still being able to evaluate it in a reasonable time frame, is a simulated annealing approach. This is a probabilistic method designed to find approximated global optima in a large search space, by trying nearby random parameter choices with gradually smaller jumps. Another potential improvement to our parameter study could be to optimize the combined GO+SI instead of optimizing GO and SI individually.

There are also many approaches used in the literature that could potentially improve our descriptor. PCA could be used to reduce the dimensionality, allowing for a higher number of cells and bins. Alternatives to the shape index and curvedness as mentioned in \Cref{sec:valueMagnitudeFunctions} could be thoroughly tested as well as higher than second order differential structure for use as bin value and magnitude functions. We could extend our descriptor to represent structure from multiple scales like the galaxy descriptor \Citet{pedersen2013shape}. This is however not necessarily a good idea in combination with omitting smoothing for pedestrian detection, since the information will largely be the same. Two-way matching, where we consider the distance ratio between best matches in both images, could improve image correspondence results. However, it is not important for comparing the performance of descriptors to each other. Higher level descriptors such as bag-of-words models or Fisher vectors \cite{sanchez2013image} could be used to perform object detection based on distributions of our descriptors rather than the descriptors themselves.

\subbibliography

\end{document}
