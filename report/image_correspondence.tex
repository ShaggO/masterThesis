\documentclass[thesis.tex]{subfiles}
\newcommand\TPR{\mathit{TPR}}
\newcommand\FPR{\mathit{FPR}}
\newcommand\OMP{\mathit{1-P}}
\newcommand\TP{\mathit{TP}}
\newcommand\FP{\mathit{FP}}
\newcommand\TN{\mathit{TN}}
\newcommand\FN{\mathit{FN}}
\newcommand\ROC{ROC}
\newcommand\PR{PR}
\def\x{\mathbf{x}}

\begin{document}
\chapter{Image correspondence}
\label{sec:ic}
%
In this chapter we explain the image correspondence problem and the application of our descriptor to solve it.

Image correspondence is the problem of matching features across two images $A$ and $B$ of the same object. A match should ideally indicate that the two features correspond to the same physical point or object. The two images of the object can be captured under different lighting conditions, taken from different positions, and have varying tilt, rotation, zoom, and focus; hence the images will differ based on these variables. By using an interest point detector in combination with a descriptor, we are able to find interest points in $A$ and $B$, describe each interest point with a descriptor algorithm, and compare these descriptors across the two images by a given similarity measure to estimate whether the points match.

The chapter is structured as follows. First we describe the steps of our approach: the strategy to select matches between images, the similarity measure used for this, and the the measure used to evaluate matching performance. We then describe the dataset and how to use it to determine correctness of matches. Next, we specify how we apply our descriptor to the problem as well as our experimental setup. In order to visualize our approach, we then extend the earlier example with image correspondence. Finally, we present our parameter optimization and test results, which include a comparison with SIFT.
%
\section{Matching strategies}
\label{sec:matching_strategies}
%
A matching strategy is a method of picking potential matches between images and assigning them a score denoting how confident we are in the match. If the confidence score is below a chosen threshold $t$, the match is positive. Otherwise it is a negative match and rejected.
{mikolajczyk2005performance} described the following three matching strategies that are used when solving the image correspondence problem:
Simple \emph{thresholding} compares each descriptor in image $A$ with each descriptor in image $B$ to find potential matches. The confidence score is defined as the mutual distance between each pair of descriptors. \emph{Best-thresholding} uses the same confidence score as the thresholding strategy, but only the best matching descriptors in $B$ for each descriptor in $A$ are considered. \emph{Ratio-thresholding} also considers only the best matching descriptors. The distances $B_1$ and $B_2$ between each descriptor in $A$ and the two best matching descriptors in $B$ are computed, and the ratio $\frac{B_1}{B_2}$ between the two distances is used as confidence score.

We choose the ratio-thresholding strategy since this is the one favoured in previous image correspondence evaluations \cite{mikolajczyk2005performance,dahl2011finding,larsen2012jet,lowe2004distinctive}. Additionally this matching strategy is more general since it uses the ratio between the distance to the two best matching features instead of the absolute values giving less dependence on the descriptor in question. The two following cases give a good rationale for choosing this matching strategy: The first case is when two descriptors are good matches. In this case the descriptors will be very similar, and their difference could be caused by noise or other small variations. Due to this fact the algorithm shouldn't choose one over the other, and hence the matching is negative since the distance ratio is close to 1. The second case is when only poorly matching descriptors are present. In this case the distance to both of the best matching descriptors will be high and the distance ratio close to 1 giving us a negative match as desired.

When using the approach in practice, a threshold $t$ is chosen depending on the precision needed, and only the positive matches are extracted.


\section{Similarity measures}
\label{sec:similarity_measure}
We need a similarity measure in order to compute the distance between two descriptors. Recall that descriptors are vectors of real numbers. The Euclidean distance is widely used as similarity measure in the literature \cite{lowe2004distinctive,ke2004pca,mikolajczyk2005performance}. \citet{larsen2012in} evaluated the following different similarity measures for histogram based descriptors similar to ours: $L_1$ distance, Euclidean distance, $\chi^2$ distance, Kullback-Leibler divergence, and Jensen-Shannon divergence, and he found no notable difference in performance. Furthermore preliminary tests on our descriptor show that Euclidean distance ($L_2$) slightly outperforms $L_1$-distance. Therefore we choose to use Euclidean distance.

\section{Performance measures}
\label{sec:performance_measures}

Having established the matching strategy and similarity measure, we now describe the two performance measures which we use for evaluation of descriptor algorithms.

Assume that we have computed the descriptor distances, found the two best matching descriptors in $B$ for each descriptor in $A$ and computed their distance ratio $r$. We now wish to compute the performance of the descriptor algorithm utilized. The problem has been reduced to a binary classification problem of whether two descriptors match or not. Following the approach described in \Cref{sec:binaryClassificationMeasures}, using the distance ratio $r$ as the classification score $s$, we are able to compute the PR- and ROC-curves as well as their AUCs to get a measure of the performance of our descriptor.

Recall the discussion of the benefits of ROC and PR in \Cref{sec:binaryClassificationMeasures}. The PR measure is beneficial to use when the positive class is of greater importance than the negative class. In the image correspondence problem we wish to find as many corresponding points as possible. However negative matches are not used, and therefore we don't want to emphasize the true negatives in our performance measure. This means that we would rather compare the number of false negatives with true positives, as recall does, instead of with true negatives, as false positive rate does. Therefore the PR measure has greater importance than the ROC measure.

%The approach is similar to that of \citet{winder2009picking,dahl2011finding}.

\section{Dataset}
\label{sec:dtuDataset}
The dataset, which we use for training and evaluation of our descriptor, is called the \emph{DTU Robot 3D dataset} \cite{aanaes2010recall} (from now on called the DTU dataset). It consists of images of objects taken in a closed black box with varying illumination using 19 fixed position LED lights. In total there are 60 scenes with varying objects. \Cref{fig:dtu_examples} shows 4 example scenes. \citet{aanaes2010ground} classify the scenes into categories as shown in \Cref{tbl:dtu_scene_classifications}.

\begin{figure}[p]
	\centering
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\textwidth]{img/dtu_example_1.png}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\textwidth]{img/dtu_example_2.png}
	\end{subfigure}
	\par\medskip
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\textwidth]{img/dtu_example_3.png}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\textwidth]{img/dtu_example_4.png}
	\end{subfigure}
	\caption{Example scenes from the DTU dataset}
	\label{fig:dtu_examples}
%
\vspace{1cm}
%
	\centering
	\begin{tabular}{l l r}
		\toprule
		Class & Scene numbers & Total \\
		\midrule
		House					& 1, 4, 8, 31, 32, 49, 50, 55				& 8 \\
		Books					& 2, 11, 20, 21								& 4 \\
		Fabric					& 5, 6, 45, 46, 47, 48						& 6 \\
		Greens					& 23, 24, 25, 26, 27, 51, 52, 53, 54, 56	& 10 \\
		Beer  					& 15, 16									& 2 \\
		Teddy Bears 			& 9, 10, 43, 44								& 4 \\
		Building Materials 		& 33, 34, 35, 36, 37						& 5 \\
		Decorative Items (Art) 	& 38, 39, 40, 41, 42						& 5 \\
		Groceries 				& 12, 28, 29, 30							& 4 \\
		Twigs and Leaves 		& 17, 57, 58, 59, 60 						& 5 \\
		\bottomrule
	\end{tabular}
	\caption{Scene object classifications from \cite[Table 1]{aanaes2010ground}}
	\label{tbl:dtu_scene_classifications}
\end{figure}

The camera is positioned around each scene using an industrial robot arm, which has automatically captured each scene from 119 positions. These positions are defined from a fixed frontal view varying the viewpoint $\theta$ in three arcs at different distance $d$ to the scene. Arc 1 has $d = \SI{0.5}{\meter}$ to the scene and $\theta$ spans $\SI{\pm40}{\degree}$, arc 2 has $d = \SI{0.65}{\meter}$ and $\theta$ spans $\SI{\pm25}{\degree}$, and arc 3 has $d = \SI{0.8}{\meter}$ and $\theta$ spans $\SI{\pm20}{\degree}$. Furthermore a linear path is captured by moving the camera away from the scene, which corresponds to zooming or scaling the scene. This is done at $\theta = \SI{0}{\degree}$ and $d$ spans $[\SI{0.5}{\meter};\SI{0.8}{\meter} ]$. At each of the 119 camera positions 19 individual images $I_i$ are taken with each of the LED lights $i,~\text{for}~i = 1,\hdots,19$ turned on. Using the given camera positions we get four camera paths: three arc paths and one linear path. \Cref{fig:dtu_overview} shows an overview of these camera paths. When computing the performance across the paths we wish to solve the image correspondence problem for matching each image in each path with the \emph{key frame} image (middle position on arc 1), since this gives us the performance when varying the viewing angle and scale in a structured manner.

\begin{figure}[p]
	\centering
	\includegraphics[width=0.9\textwidth]{img/CameraPosbOwn.pdf}
	\caption{Overview of camera positions and paths in the DTU dataset. The red positions mark training camera positions, the arrows mark camera positions of the light path experiments where only the ones marked with blue are used for training. Reproduced and improved illustration from \citet[Figure 3, pp. 3]{aanaes2010recall}.}
	\label{fig:dtu_overview}
%\end{figure}
%\begin{figure}[tb]
	\vspace{1cm}
	\centering
	\includegraphics[width=0.7\textwidth]{img/dtu_light_overview.pdf}
	\caption{Light positions of the x- and z-axis light paths. Training light positions marked with red. Reproduced and improved figure from \citet[Fig. 1, pp. 644]{larsen2012in}.}
	\label{fig:dtu_light_overview}
\end{figure}

The choice of capturing the scenes with individual LED lights created images with a high amount of cast shadows. \citet{larsen2012jet} created a set\footnote{dataset available at \url{http://roboimagedata.imm.dtu.dk/data/condensed.tar.gz}} of artificial diffuse and light paths from the individual LED images which we now briefly explain. The artificial diffuse light images are created from the individual lightings in order to only evaluate the performance of our descriptor under viewpoint changes and to get more natural images. These diffuse images are created by averaging over the individual light images for each camera position:
\begin{align}
	I_{\text{diffuse}} = \frac{1}{19} \sum_{i = 1}^{19} I_{i}
\end{align}
Since the dataset consists of individual LED images, one is able to construct images simulating two light source paths going from right to left and back to front respectively.
Given a light position $\x$ in the spatial domain of the LED positions, the image $I_\x$ is constructed by weighting each LED image by the Gaussian of the distance to $\x$:
\begin{align}
	I_{x} = \sum_{i = 1}^{19} G(\x - \x_i,\sigma) I_{i}
\end{align}
\Cref{fig:dtu_light_overview} shows the selected light positions. In order to get a somewhat general measurement for the robustness against light variations, there are light paths generated for the following four image positions: 12 (arc 1), 25 (arc 1), 60 (linear path), and 87 (arc 2). The light path images are compared to the key frame image with diffuse light, which gives a difference in lighting conditions.
See \citet{aanaes2010recall,aanaes2010ground,larsen2012jet} for more information about the dataset and the generated light paths.

\Cref{fig:light_example} show 6 of the different light images for scene 4 at camera position 60. \Cref{fig:light_example_02,fig:light_example_17,fig:light_example_08} show the images taken with individual LED lighting (numbers 2, 17, and 8 respectively), and \Cref{fig:light_example_00} shows the diffuse light image. We here notice the significant difference in cast shadows using the three LEDs individually compared to the diffuse light image. The diffuse light image is however quite dark compared to the LED 8 light image, which could potentially cause problems in some scenes generating too few interest points.
\Cref{fig:light_example_28,fig:light_example_20} show the left- and rightmost positions of the $x$-axis light path. By comparing these to their LED counterparts (\Cref{fig:light_example_02,fig:light_example_17} respectively), we see that the cast shadows are less significant and therefore look more natural.
\Cref{fig:viewpoint_example} shows 3 different camera positions for scene 4 including the key frame (position 25).
%
\begin{figure}[tb]
	\centering
	\begin{subfigure}[t]{0.32\textwidth}
		\includegraphics[width=\textwidth]{img/scene_04_img60_08.png}
		\caption{LED 8 light}
		\label{fig:light_example_08}
	\end{subfigure}
	\begin{subfigure}[t]{0.32\textwidth}
		\includegraphics[width=\textwidth]{img/scene_04_img60_02.png}
		\caption{LED 2 light}
		\label{fig:light_example_02}
	\end{subfigure}
	\begin{subfigure}[t]{0.32\textwidth}
		\includegraphics[width=\textwidth]{img/scene_04_img60_17.png}
		\caption{LED 17 light}
		\label{fig:light_example_17}
	\end{subfigure}
	\begin{subfigure}[t]{0.32\textwidth}
		\includegraphics[width=\textwidth]{img/scene_04_img60_00.png}
		\caption{Diffuse light}
		\label{fig:light_example_00}
	\end{subfigure}
	\begin{subfigure}[t]{0.32\textwidth}
		\includegraphics[width=\textwidth]{img/scene_04_img60_28.png}
		\caption{Leftmost position of the $x$-axis light path}
		\label{fig:light_example_28}
	\end{subfigure}
	\begin{subfigure}[t]{0.32\textwidth}
		\includegraphics[width=\textwidth]{img/scene_04_img60_20.png}
		\caption{Rightmost position of the $x$-axis light path}
		\label{fig:light_example_20}
	\end{subfigure}
	\caption{Examples of light images in scene 4 at camera position 60.}
	\label{fig:light_example}
\end{figure}
%
\begin{figure}[tb]
	\centering
	\begin{subfigure}[t]{0.32\textwidth}
		\includegraphics[width=\textwidth]{img/scene_04_img12_00.png}
		\caption{Camera position 12 (arc 1)}
		\label{fig:viewpoint_example_left}
	\end{subfigure}
	\begin{subfigure}[t]{0.32\textwidth}
		\includegraphics[width=\textwidth]{img/scene_04_img25_00.png}
		\caption{Camera position 25 (key frame)}
		\label{fig:viewpoint_example_keyframe}
	\end{subfigure}
	\begin{subfigure}[t]{0.32\textwidth}
		\includegraphics[width=\textwidth]{img/scene_04_img64_00.png}
		\caption{Camera position 64 (linear path)}
		\label{fig:viewpoint_example_scale}
	\end{subfigure}
	\caption{Examples of various camera positions of scene 4 using diffuse light.}
	\label{fig:viewpoint_example}
\end{figure}
%
\subsection{Evaluation}
In order to be able to classify matches between two images $A$ and $B$ as true or false, we need an evaluation method. In other words we need to generate the match ground truth. The DTU dataset has been scanned using \emph{structured light} generating point clouds for the surfaces in each image. Using these points and the known camera positions for each image, we are able to check if a matching of two points from $A$ and $B$ indeed correspond to 3D points close enough to one another to be true. \citet{aanaes2010recall} defined three evaluation criteria, of which we will be using the two first as chosen in \citet{larsen2012in}: epipolar and surface geometry. We first define the property of being 3D reconstructable: An image point is said to be 3D reconstructable if there exists a point from the surface point cloud within a 10 pixel window in the image plane. This corresponds to a pixel within approximately 6 mm in the surface point cloud. Sometimes a matching is made for a point which is not 3D reconstructable. In this case no ground truth can be calculated, and the matching is ignored in the performance evaluation. The following two criteria (illustrated in \Cref{fig:icEvalGT}) determine the matching correspondence ground truth of the two points:
\begin{itemize}
	\item \textbf{Epipolar geometry consistency:} Given a point in image $A$, the point in image $B$ should be within a 2.5 pixel orthogonal distance to the epipolar line of the first point as illustrated in \Cref{fig:icEval1GT}.
	\item \textbf{Surface geometry consistency:} The points need to be 3D reconstructable, and they need to be within 3 mm of each other in the surface point cloud as illustrated in \Cref{fig:icEval2GT}.
\end{itemize}
The evaluation described in this section has been implemented by \citet{aanaes2010recall}. In this thesis we have used and improved their implementation for our evaluation of descriptor performance.

\begin{figure}[tb]
	\centering
	\begin{subfigure}[t]{0.8\textwidth}
		\includegraphics[width=\textwidth]{img/icEval1GT.pdf}
		\caption{Epipolar geometry consistency: Corresponding points should have an orthogonal distance to each others epipolar lines of maximum 2.5 pixel.}
		\label{fig:icEval1GT}
	\end{subfigure}
	\begin{subfigure}[t]{0.8\textwidth}
		\includegraphics[width=\textwidth]{img/icEval2GT.pdf}
		\caption{Surface geometry consistency: Corresponding descriptors should be 3D reconstructable and be within 3 mm of each other in the surface point cloud.}
		\label{fig:icEval2GT}
	\end{subfigure}
	\caption{Illustration of the two ground truth evaluation criteria. Figure reproduced from \cite[Figure 5 (a-b),pp. 4]{aanaes2010recall}.}
	\label{fig:icEvalGT}
\end{figure}

%
\subsection{Pitfalls and deficiencies}
The DTU dataset is built to test the ability of computer vision systems to cope with viewpoint changes, scaling, light changes, and to a certain extent occlusions as well. Since all images are captured with the same camera tilt, no rotation of objects occurs, and hence rotational invariance and robustness cannot be evaluated using this dataset.

\Cref{fig:dtu_problems} shows the problems we have found within the DTU dataset, which we will go through here.

The artificial diffuse light is, as mentioned in the previous section, created by combining images captured under individual LED lighting. This is not optimal as some of the images are visually suffering from the spatial layout of the LED lights. \Cref{fig:dtu_problems_diffuse} shows image 20 (arc 1) from set 2 of the dataset. The diffuse light problem is clearly visible in the cast shadows seen on the 1st and 3rd book in the stack of lying books. Ideally these shadows would form either a smoothed or a single hard shadow edge instead of a number of gradually fading hard edges.

The automatic capturing of images using a robot arm combined with the individual LED lighting has the side-effect of the robot arm casting shadows in some of the images. This is seen when using the 3rd LED light and having captured the scenes from the far left of arc 1. \Cref{fig:dtu_problems_robot} shows an example of a cast shadow originating from the robot arm. This is however only a problem with one of the 19 LED light images, and hence the effect on the final test images is minimal.
%
\begin{figure}[tb]
	\centering
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\textwidth]{img/diffuse_light_problem.png}
		\caption{Diffuse lighting in set 2, image 20, with shadow artifacts}
		\label{fig:dtu_problems_diffuse}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=\textwidth]{img/robot_arm_shadow.png}
		\caption{Robot arm shadow in set 4, image 7, LED light 3}
		\label{fig:dtu_problems_robot}
	\end{subfigure}
	\caption{Examples of problematic images in the DTU dataset}
	\label{fig:dtu_problems}
\end{figure}
%
\section{Application of descriptors}
\label{sec:icApplication}
%
Recall that we introduced the GO, SI, and GO+SI descriptors in \Cref{sec:valueMagnitudeFunctions}.
We will use the DTU dataset to evaluate these descriptors on the image correspondence problem. Our descriptors naturally use the interest point cell layouts for this application, described in \Cref{sec:cellApertureFunctionPoint}. Additionally we will evaluate SIFT in order to compare our results. Since DTU images have a fixed orientation, we disable SIFT's orientation estimation step, which our descriptors don't have either. We have tested and verified that using orientation estimation decreases performance. As mentioned in \Cref{sec:opponentColourSpace}, the descriptors will be computed for opponent colour channels and concatenated, which slightly improves the results. Each descriptor will be computed for the same interest points from a multi-scale DoG detector, described in \Cref{sec:multiscaleDoG}. This ensures that only the choice of descriptor can affect the result. Training is done on grayscale images, whereas testing is done on the images converted to opponent colour space as mentioned in \Cref{sec:opponentColourSpace}.
%
\section{Experimental setup}
%
The DTU dataset is constructed using four different camera movement paths: Arc 1 through 3 and the linear path described in \Cref{sec:dtuDataset}. We therefore choose to perform one experiment for each of these paths, where we average across the different scenes for each camera position. These four experiments are all conducted using the diffuse light images. Furthermore we conduct two light experiments, where we move the light in the x- and z-axis respectively. For these experiments we both average across scenes and a fixed set of camera positions. In total we have six different experiments with perspective, scale and light transformations.

Since the DTU dataset is not split in a test and a train part, there is no pre-defined way of training and testing an algorithm on the dataset. \citet{larsen2012jet} used the Oxford Affine Covariance Region dataset\footnote{\url{http://www.robots.ox.ac.uk/~vgg/research/affine}} for manual parameter optimization of their Jet-based local image descriptors. This dataset is however very small and limited, and hence we risk severe overfitting to the dataset if used for training. Instead we choose to split the DTU dataset with respect to the scenes into six parts for the purpose of 6-fold cross-validation. For each fold we thus have 50 training scenes and 10 testing scenes.

Testing on a scene is done on the complete camera and light paths, whereas we only perform training on a small subset of these, since the dataset contains quite a large amount of images to be processed. \Cref{fig:dtu_overview} shows the camera positions of the dataset as described in \Cref{sec:dtuDataset}. In this illustration the training camera positions for the three arcs and the linear path are marked with red. The camera positions for the light experiments are marked with arrows, where only the ones marked with blue arrows are used for training. \Cref{fig:dtu_light_overview} shows the light positions for the light experiments. The red pluses and circles mark the $x$- and $z$-axis light path images used for training.
%
\section{Example}
%
In this section we will continue the example from \Cref{sec:proposedDescriptorExample} by extending it to show the results of solving the image correspondence problem between the image of the example and the corresponding key frame image from the DTU dataset. For this extension of the example we only use the GO descriptor.

\Cref{fig:imageCorrespondenceMatches} shows matches between the descriptors from the two images with score $s$ below threshold $t = 0.8$. The green and red lines indicate true and false positive classifications respectively. The images contain several similar beer cans, and hence many of the false positive classifications are caused by patterns that are visible in multiple of the cans.
%
\begin{figure}[p]
	\centerline{\includegraphics[width=1.1\textwidth]{img/imageCorrespondenceMatches.pdf}}
	\caption{Matches with score $s$ below threshold $t = 0.8$. Condition positive and negative matches are shown in green and red, respectively.}
	\label{fig:imageCorrespondenceMatches}
	\vspace{5mm}
	%
	\centerline{\includegraphics[width=1.1\textwidth]{img/imageCorrespondenceCorrectMatch.pdf}}
	\caption{Example of a condition positive match. First (1) and second (2) best matches have descriptor distances $0.076$ and $0.083$, respectively, resulting in a distance ratio of $0.92$.}
	\label{fig:imageCorrespondenceCorrectMatch}
	\vspace{5mm}
	%
	\centerline{\includegraphics[width=1.1\textwidth]{img/imageCorrespondenceIncorrectMatch.pdf}}
	\caption{Example of an condition negative match. First (1) and second (2) best matches have descriptor distances $0.079$ and $0.081$, respectively, resulting in a distance ratio of $0.97$.}
	\label{fig:imageCorrespondenceIncorrectMatch}
\end{figure}
%
\begin{figure}[tb]
	\centering
	\begin{subfigure}[t]{0.49\textwidth}
		\includegraphics[width=\textwidth]{img/imageCorrespondenceROC.pdf}
		\caption{ROC-curve (blue line), curve of no predictive value attained by classifying randomly (dashed line), and area under the ROC-curve (light blue area)}
		\label{fig:imageCorrespondenceROC}
	\end{subfigure}
	\begin{subfigure}[t]{0.49\textwidth}
		\includegraphics[width=\textwidth]{img/imageCorrespondencePR.pdf}
		\caption{PR-curve (blue line), area under the PR-curve (light blue area)}
		\label{fig:imageCorrespondencePR}
	\end{subfigure}
	\caption{Examples of evaluation measure curves}
	\label{fig:imageCorrespondenceCurves}
\end{figure}

Recall from \Cref{sec:matching_strategies} that the chosen matching strategy finds a single correspondence candidate for each feature, and the match is evaluated based on the distance ratio $r$ between the best and second best correspondence candidates. \Cref{fig:imageCorrespondenceCorrectMatch,fig:imageCorrespondenceIncorrectMatch} show two examples of a true positive match and a true negative match respectively. The green line shows a condition positive match, and the red lines show condition negative matches. In \Cref{fig:imageCorrespondenceCorrectMatch} we see that there indeed is a correspondence between the best match of the two images with distance 0.083, and that our matching strategy results in a distance ratio (score) $s$ of 0.92. In \Cref{fig:imageCorrespondenceIncorrectMatch} we see that there is no match between the best matching descriptors of each image with distance 0.081. For this match $s$ is computed to 0.97. If we wanted to classify both matches correctly, we should therefore set our threshold $t$ between 0.92 and 0.97. If we had used the distance as similarity measure, we would not have been able to classify both correctly, since the distance of the condition negative match is lower than the distance of the condition positive match.

When having computed the distance ratios of all matches, we are able to construct the ROC- and PR-curves giving us two measures of the performance of the descriptor matching. \Cref{fig:imageCorrespondenceCurves} shows the ROC-curve \subref{fig:imageCorrespondenceROC} and PR-curve \subref{fig:imageCorrespondencePR} as well as the area under the two curves. The match has a ROC AUC of 0.72 and PR AUC of 0.63. From the ROC plot we see that the descriptor matching performs better than a random classification since the ROC-curve (blue line) lies above the curve of no predictive value (dashed line).
%
\section{Parameter study}
\label{sec:icParameterStudy}
%
%Test: 6 fold cross validation.
%Tune on 5 folds, test on 1.
%
%Notes:
%
%1. Iterative minimization
%2. Computationally heavy, many parameters, hard to say something about the correlation of the parameters:
%3. Each sub-optimization:
%3a. Enum: One of the enumerated choices
%3b. Zoom: (2 iterations, initially large interval, then small zoomed in interval)
%4. 
%
%1. Problems with boosting the noise when using pixel-wise normalization?
We wish to optimize the descriptor parameters listed in \Cref{sec:descriptorParameters} with the objective of maximizing average matching PR AUC. We do this separately for GO and SI as they may have different optimal parameters, and simply combine these to create the GO+SI descriptor. As mentioned in the experimental setup, we have split the DTU dataset into six parts to reduce overfitting by 6-fold cross-validation. For each fold we optimize parameters on five parts and test the optimized descriptor on the sixth part. This means we obtain six sets of parameters and a single PR AUC for each image, where the images are evaluated without optimizing across their respective scenes. To save time evaluating parameters, we scale the images to half size and convert them to grayscale. The testing is done on the full scale images converted to opponent colour space as mentioned in \Cref{sec:opponentColourSpace}.

Our actual parameter optimization is problematic because we need to optimize ten parameters at once, and each evaluation is computationally heavy. Our basic strategy is thus to cycle through each parameter and optimize it separately while fixing the other parameters. We do this iteratively until convergence.
The risk of this strategy is getting stuck in local optima. To avoid this, recall that we have defined the parameters to reduce correlation: e.g. when bin count $n$ is increased, the width of the bins is automatically reduced to compensate. We also select certain correlated parameters to optimize together: grid type with grid size, as well as cell and bin kernels with their respective scales. Furthermore we optimize continuous parameters in two steps: first a coarse search and then a finer search around the best result.

The optimal parameters of our parameter study for our GO and SI descriptors are shown in \Cref{fig:ICparamsGo,fig:ICparamsSi} respectively.
From these results we see that most of the parameters are quite stable across the folds.

We first compare the grid type and size of the GO and SI descriptors. SI and a single of the GO folds use the concentric polar central (CPC) grid, whereas the five other GO folds use the polar central (PC) grid. Both descriptors use grid size $12 \times 2$. Note that the amount of angular cells is higher than for GLOH and DAISY which use 8 angular cells. \Cref{tbl:dtuLayoutParametersGo,tbl:dtuLayoutParametersSi} show the average PR AUC on training data for GO and SI respectively for various choices of grid parameters. We see that the $12\times2$ PC and CPC grids have nearly equal average PR AUC for both GO and SI, and hence the choice between these two grid types only gives a marginal performance difference. By looking at the grid results, we see that the performance difference between a concentric grid and the corresponding non-concentric grid is very small. The difference between log-polar (LP) and concentric log-polar (CLP) is however significant, which is caused by a more dense grid when using CLP than LP.
The polar grids with a central cell perform slightly better than their corresponding polar grids without a central cell. Furthermore log-polar grids are always worse than the corresponding polar grids.

\Cref{fig:dtuParametersGoAuc,fig:dtuParametersSiAuc} show average PR AUC on training data for GO and SI respectively when adjusting the parameters relative to the optimized parameters. Furthermore \Cref{fig:dtuParametersGoDims,fig:dtuParametersSiDims} show the dimensionality of GO and SI respectively when varying the bin count $n$. On each of these graphs, the optimal choice of each parameter is marked with a cross. From the figures we see that the grid radius $r$ as well as cell and bin kernels and scales are the parameters that affect performance the most for both GO and SI.

% The bin count $n$ parameters for GO (\Cref{fig:dtuParametersGo_binCount}) and SI (\Cref{fig:dtuParametersSi_binCount}) behave differently. For GO the performance increases for increasing $n$ and flattens out around 12-13, whereas performance drops significantly when going in either direction from $n = 8$ for SI.

Looking at the cell and bin kernels, we see that both GO and SI use Gaussian kernels. From \Cref{fig:dtuParametersGo_cellSigma,fig:dtuParametersGo_binSigma} we see that the differences in maximum performance between Gaussian and triangle kernels are very small. By choosing triangle kernels instead of Gaussian ones, we could save computations, since the two kernels have different support radius in practice. This alternative choice is viable since the optimal $\alpha$ and $\beta$ for the triangle kernels are only slightly higher than for the Gaussian kernels.
This holds for SI as well as seen in \Cref{fig:dtuParametersSi_cellSigma,fig:dtuParametersSi_binSigma}.

For both descriptors the center scale $\rho$ has little importance once it reaches a certain minimal value for each descriptor as can be seen in \Cref{fig:dtuParametersGo_centerSigma,fig:dtuParametersSi_centerSigma} for GO and SI respectively. This is the reason why the optimal $\rho$ is unstable across the folds for GO.

%
\begin{table}[p]
\centerline{
\begin{tabular}{ l c c c c c c c}
\toprule
{} & \multicolumn{6}{c}{Cross-validation fold} \\
Parameter & 1 & 2 & 3 & 4 & 5 & 6 & chosen \\ \midrule
Grid type & PC & CPC & PC & PC & PC & PC & \textbf{PC} \\
Grid size & $12 \times 2$ & $12 \times 2$ & $12 \times 2$ & $12 \times 2$ & $12 \times 2$ & $12 \times 2$ & \textbf{$\mathbf{12 \times 2}$} \\
Grid radius $r$ & $13.5$ & $13.0$ & $13.5$ & $13.5$ & $13.5$ & $13.5$ & $\mathbf{13.5}$ \\
Center scale $\rho$ & $1.4$ & $1.9$ & $1.5$ & $1.4$ & $1.7$ & $1.5$ & $\mathbf{1.6}$ \\
Cell kernel & \textit{G} & \textit{G} & \textit{G} & \textit{G} & \textit{G} & \textit{G} & \textbf{\textit{G}} \\
Cell scale $\alpha$ & $0.8$ & $0.9$ & $0.8$ & $0.8$ & $0.8$ & $0.8$ & $\mathbf{0.8}$ \\
Bin count $n$ & $12$ & $12$ & $12$ & $14$ & $12$ & $14$ & $\mathbf{12}$ \\
Bin kernel & \textit{G} & \textit{G} & \textit{G} & \textit{G} & \textit{G} & \textit{G} & \textbf{\textit{G}} \\
Bin scale $\beta$ & $1.3$ & $1.3$ & $1.3$ & $1.3$ & $1.3$ & $1.3$ & $\mathbf{1.3}$ \\
Norm. scale $\eta$ & $1.6$ & $1.6$ & $1.6$ & $1.6$ & $1.6$ & $1.8$ & $\mathbf{1.6}$ \\
\bottomrule
\end{tabular}}
\caption{Optimized parameters from our parameter study for GO. Separate sets of parameters are found for each fold, and we manually choose a set of recommended parameters based on these.}
\label{fig:ICparamsGo}
%
\vspace{1cm}
%
\centerline{
\begin{tabular}{ l c c c c c c c}
\toprule
{} & \multicolumn{6}{c}{Cross-validation fold} \\
Parameter & 1 & 2 & 3 & 4 & 5 & 6 & chosen \\ \midrule
Grid type & CPC & CPC & CPC & CPC & CPC & CPC & \textbf{CPC} \\
Grid size & $12 \times 2$ & $12 \times 2$ & $12 \times 2$ & $12 \times 2$ & $12 \times 2$ & $12 \times 2$ & \textbf{$\mathbf{12 \times 2}$} \\
Grid radius $r$ & $13.5$ & $14.0$ & $13.5$ & $14.0$ & $14.0$ & $14.0$ & $\mathbf{14.0}$ \\
Center scale $\rho$ & $1.9$ & $2.0$ & $2.0$ & $2.0$ & $2.0$ & $2.0$ & $\mathbf{2.0}$ \\
Cell kernel & \textit{G} & \textit{G} & \textit{G} & \textit{G} & \textit{G} & \textit{G} & \textbf{\textit{G}} \\
Cell scale $\alpha$ & $1.1$ & $1.0$ & $1.1$ & $1.0$ & $1.0$ & $1.0$ & $\mathbf{1.0}$ \\
Bin count $n$ & $8$ & $8$ & $8$ & $8$ & $8$ & $8$ & $\mathbf{8}$ \\
Bin kernel & \textit{G} & \textit{G} & \textit{G} & \textit{G} & \textit{G} & \textit{G} & \textbf{\textit{G}} \\
Bin scale $\beta$ & $2.0$ & $2.0$ & $2.0$ & $2.0$ & $2.0$ & $2.0$ & $\mathbf{2.0}$ \\
Norm. scale $\eta$ & $2.6$ & $2.8$ & $2.6$ & $2.6$ & $2.6$ & $2.6$ & $\mathbf{2.6}$ \\
\bottomrule
\end{tabular}}
\caption{Optimized parameters from our parameter study for SI. Separate sets of parameters are found for each fold, and we manually choose a set of recommended parameters based on these.}
\label{fig:ICparamsSi}
\end{table}
%
\begin{table}[tb]
\centering
\begin{tabular}{ c c c c c c c }
\toprule
Grid size & P & CP & PC & CPC & LP & CLP \\ \midrule
$6 \times 2$ & $0.756$ & $0.756$ & $0.759$ & $0.759$ & $0.700$ & $0.732$ \\ 
$8 \times 2$ & $0.767$ & $0.767$ & $0.770$ & $0.770$ & $0.759$ & $0.763$ \\ 
$10 \times 2$ & $0.772$ & $0.772$ & $0.776$ & $0.776$ & $0.769$ & $0.769$ \\ 
$12 \times 2$ & $0.775$ & $0.775$ & $\mathbf{0.779}$ & $0.779$ & $0.770$ & $0.767$ \\ 
$6 \times 3$ & $0.763$ & $0.764$ & $0.770$ & $0.770$ & - & - \\ 
$8 \times 3$ & $0.771$ & $0.771$ & $0.777$ & $0.777$ & - & - \\ 
$6 \times 4$ & $0.758$ & $0.759$ & $0.767$ & $0.767$ & - & - \\ 
\bottomrule
\end{tabular}
\caption{Average matching performance (PR AUC) on training data for GO when adjusting the grid parameters. Optimal performance is marked in bold.}
\label{tbl:dtuLayoutParametersGo}
\end{table}
%
\begin{table}[tb]
\centering
\begin{tabular}{ c c c c c c c }
\toprule
Grid size & P & CP & PC & CPC & LP & CLP \\ \midrule
$6 \times 2$ & $0.684$ & $0.685$ & $0.691$ & $0.692$ & $0.633$ & $0.664$ \\ 
$8 \times 2$ & $0.712$ & $0.713$ & $0.720$ & $0.721$ & $0.707$ & $0.712$ \\ 
$10 \times 2$ & $0.729$ & $0.731$ & $0.739$ & $0.739$ & $0.726$ & $0.727$ \\ 
$12 \times 2$ & $0.738$ & $0.739$ & $0.748$ & $\mathbf{0.749}$ & $0.732$ & $0.729$ \\ 
$6 \times 3$ & $0.711$ & $0.713$ & $0.717$ & $0.718$ & - & - \\ 
$8 \times 3$ & $0.731$ & $0.733$ & $0.736$ & $0.738$ & - & - \\ 
$6 \times 4$ & $0.706$ & $0.707$ & $0.721$ & $0.720$ & - & - \\
\bottomrule
\end{tabular}
\caption{Average matching performance (PR AUC) on training data for SI when adjusting the grid parameters. Optimal performance is marked in bold.}
\label{tbl:dtuLayoutParametersSi}
\end{table}
%
\begin{figure}[p]
\centering
	\centerline{\begin{subfigure}[t]{0.593\textwidth}
		\includegraphics[width=\textwidth]{img/dtuParametersGo_gridRadius.pdf}
		\caption{Grid radius $r$}
		\label{fig:dtuParametersGo_gridRadius}
	\end{subfigure}
	\begin{subfigure}[t]{0.593\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/dtuParametersGo_binCount.pdf}
		\caption{Bin count $n$}
		\label{fig:dtuParametersGo_binCount}
	\end{subfigure}}
	\centerline{\begin{subfigure}[t]{0.593\textwidth}
		\includegraphics[width=\textwidth]{img/dtuParametersGo_cellSigmaAlt.pdf}
		\caption{Cell scale $\alpha$}
		\label{fig:dtuParametersGo_cellSigma}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{0.593\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/dtuParametersGo_binSigmaAlt.pdf}
		\caption{Bin scale $\beta$}
		\label{fig:dtuParametersGo_binSigma}
	\end{subfigure}}
	\centerline{\begin{subfigure}[t]{0.593\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/dtuParametersGo_centerSigma.pdf}
		\caption{Center scale $\rho$}
		\label{fig:dtuParametersGo_centerSigma}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{0.593\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/dtuParametersGo_normSigma.pdf}
		\caption{Normalization scale $\eta$}
		\label{fig:dtuParametersGo_normSigma}
		\vspace{2mm}
	\end{subfigure}}
	\caption{Average matching performance (PR AUC) on training data for GO when adjusting the parameters. Optimal parameter values are marked with a cross.}
	\label{fig:dtuParametersGoAuc}
	\vspace{1cm}
	\includegraphics[width=0.593\textwidth]{img/dtuParametersGo_binCountDims.pdf}
	\caption{Dimensionality of GO when adjusting bin count $n$.}
	\label{fig:dtuParametersGoDims}
\end{figure}
%
\begin{figure}[p]
\centering
	\centerline{\begin{subfigure}[t]{0.593\textwidth}
		\includegraphics[width=\textwidth]{img/dtuParametersSi_gridRadius.pdf}
		\caption{Grid radius $r$}
		\label{fig:dtuParametersSi_gridRadius}
	\end{subfigure}
	\begin{subfigure}[t]{0.593\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/dtuParametersSi_binCount.pdf}
		\caption{Bin count $n$}
		\label{fig:dtuParametersSi_binCount}
	\end{subfigure}}
	\centerline{\begin{subfigure}[t]{0.593\textwidth}
		\includegraphics[width=\textwidth]{img/dtuParametersSi_cellSigmaAlt.pdf}
		\caption{Cell scale $\alpha$}
		\label{fig:dtuParametersSi_cellSigma}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{0.593\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/dtuParametersSi_binSigmaAlt.pdf}
		\caption{Bin scale $\beta$}
		\label{fig:dtuParametersSi_binSigma}
	\end{subfigure}}
	\centerline{\begin{subfigure}[t]{0.593\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/dtuParametersSi_centerSigma.pdf}
		\caption{Center scale $\rho$}
		\label{fig:dtuParametersSi_centerSigma}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{0.593\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/dtuParametersSi_normSigma.pdf}
		\caption{Normalization scale $\eta$}
		\label{fig:dtuParametersSi_normSigma}
		\vspace{2mm}
	\end{subfigure}}
	\caption{Average matching performance (PR AUC) on training data for SI when adjusting the parameters. Optimal parameter values are marked with a cross.}
	\label{fig:dtuParametersSiAuc}
	\vspace{1cm}
	\includegraphics[width=0.593\textwidth]{img/dtuParametersSi_binCountDims.pdf}
	\caption{Dimensionality of SI when adjusting bin count $n$.}
	\label{fig:dtuParametersSiDims}
\end{figure}
We have also experimented with various other parts of our descriptor that are not listed as parameters. We report some of these results for our optimal GO descriptor on training data, which has a mean PR AUC of $0.781$: It is marginally better to normalize the final descriptors before searching for matches. Omitting this step reduces the AUC by $0.002$. Recall that we resize images to half size and convert them to grayscale for training. Omitting the down-scaling step improves the AUC by $0.009$, and computing the descriptor on opponent colour channels improves the AUC by $0.014$. Omitting pixel-wise normalization reduces the AUC by $0.023$. Omitting the center aperture function reduces the AUC by $0.002$. These tendencies hold for SI as well.
%
\section{Results}
\label{sec:icResults}
%
Having found the optimal parameters for each of the 6 folds in the previous section, we are now able to compute test results for each fold and report the performance of our descriptors.
We have chosen to compare our descriptor performance against the SIFT descriptor since \citet{dahl2011finding} found the combination of DoG and SIFT to perform well.

\Cref{tbl:dtuOverallResults} shows average performance across all test images for our GO, SI, and GO+SI as well as SIFT. We see that GO+SI only has a very small advantage over GO despite adding 600 dimensions to the existing 900/1050 (varying across cross-validation folds), and that all our descriptors are marginally better than SIFT on average but have higher dimensionality as well.

The DTU dataset PR and ROC AUC test results for each path are shown in \Cref{fig:dtuResultsPR,fig:dtuResultsROC} respectively. For both these measures the GO (red), SI (green), GO+SI (blue) and SIFT (dashed grey) results are shown. Recall from \Cref{sec:binaryClassificationMeasures} that an algorithm which optimizes PR AUC does not necessarily optimize ROC AUC. Despite this fact, the mean ROC AUCs for our descriptors compared to the SIFT ROC AUC are somewhat equal to the corresponding comparisons of mean PR AUCs. SI however has a slight advantage over GO and GO+SI when visually inspecting the mean ROC AUC figures. In \Cref{sec:performance_measures} we wrote that we gave the PR measure more importance than ROC, and hence the following comparison of performance between the descriptors is based on the PR AUC figures.

The first thing we notice when looking at all the graphs in \Cref{fig:dtuResultsPR} is, that our GO+SI descriptor is marginally better than GO on arc 1, but they have identical performance on the rest of the variations. Furthermore we notice that our SI descriptor is worse than GO and GO+SI.

When looking at the three arc graphs (\Cref{fig:dtuResultsPRarc1,fig:dtuResultsPRarc2,fig:dtuResultsPRarc3}), we see that GO and GO+SI performance drops less than SIFT, when the camera is moved towards the outermost camera positions. The advantage over SIFT is however lower for arc 2 and 3 than arc 1. This difference could be caused by smaller perspective distortions at arc 2 and 3, since the distance to the camera is increased and the angular differences are smaller than for arc 1.
This hypothesis is likewise supported by the fact that all four descriptors have roughly equal performance over scale variations as shown in \Cref{fig:dtuResultsPRlinear}.

For light variations (\Cref{fig:dtuResultsPRxAxis,fig:dtuResultsPRzAxis}) our descriptors perform better than SIFT.

Having inspected the graphs for mean PR AUCs, we are now interested in knowing whether the descriptors are statistically significantly different. We test this separately for each camera and light position. Given a position, assume that the underlying distribution of PR AUCs over the scenes are normally distributed. We discuss whether this assumption is reasonable in \Cref{sec:discussionIc}. The difference between the mean PR AUC of two descriptors can be estimated by a 95\% confidence interval as described in \Cref{sec:confidenceIntervals}.
The resulting 95\% confidence interval graphs on the difference between our GO+SI descriptor and SIFT are show in \Cref{fig:dtuResultsStats}. As we see from the graphs, the confidence intervals contain 0 for all positions apart from position 1, 7, 8, and 9 of the $x$-axis light path and position 16 of the $z$-axis light path. In other words we cannot conclude that GO+SI and SIFT are significantly different except on these outermost positions of the light paths.

Performing these estimates between the remaining combinations of the GO, SI, GO+SI, and SIFT descriptors reveal that none of our descriptors are significantly different apart from GO, which is significantly better than SIFT on positions 6-9 of the $x$-axis light path and position 16 of the $z$-axis light path. Since the graphs for all these combinations are both space-consuming and not too informative once we have the above conclusions, we have omitted the graphs from the report itself, but can be found in \Cref{apx:confidenceIntervals}.
%
\begin{table}[tb]
\centering
\begin{tabular}{ l c c c }
\toprule
Descriptor & Dimensions & PR AUC & ROC AUC \\ \midrule
GO & $900/1050$ & $0.850$ & $0.888$ \\ 
SI & $600$ & $0.839$ & $0.890$ \\ 
GO+SI & $1500/1650$ & $0.851$ & $0.889$ \\ 
SIFT & $384$ & $0.831$ & $0.880$ \\ 
\bottomrule
\end{tabular}
\caption{Average performance across all test images for the various descriptors. Note that GO has a variable number of dimensions because different bin counts were found optimal across the cross-validation folds.}
\label{tbl:dtuOverallResults}
\end{table}
%
\begin{figure}[tb]
	\centerline{
		\includegraphics[width=1.2\textwidth]{img/dtuResults_opponent_legend_cropped.pdf}
	}	
	\centerline{
	\begin{subfigure}[t]{0.6242\textwidth}
		\includegraphics[width=\textwidth]{img/dtuResultsPR_opponent_1.pdf}
		\caption{Arc 1}
		\label{fig:dtuResultsPRarc1}
	\end{subfigure}
	\begin{subfigure}[t]{0.5618\textwidth}
		\includegraphics[width=\textwidth]{img/dtuResultsPR_opponent_2.pdf}
		\caption{Arc 2}
		\label{fig:dtuResultsPRarc2}
	\end{subfigure}
	}
	\centerline{
	\begin{subfigure}[t]{0.6242\textwidth}
		\includegraphics[width=\textwidth]{img/dtuResultsPR_opponent_3.pdf}
		\caption{Arc 3}
		\label{fig:dtuResultsPRarc3}
	\end{subfigure}
	\begin{subfigure}[t]{0.5618\textwidth}
		\includegraphics[width=\textwidth]{img/dtuResultsPR_opponent_4.pdf}
		\caption{Linear path}
		\label{fig:dtuResultsPRlinear}
	\end{subfigure}
	}
	\centerline{
	\begin{subfigure}[t]{0.6242\textwidth}
		\includegraphics[width=\textwidth]{img/dtuResultsPR_opponent_5.pdf}
		\caption{Light path, $x$-axis}
		\label{fig:dtuResultsPRxAxis}
	\end{subfigure}
	\begin{subfigure}[t]{0.5618\textwidth}
		\includegraphics[width=\textwidth]{img/dtuResultsPR_opponent_6.pdf}
		\caption{Light path, $z$-axis}
		\label{fig:dtuResultsPRzAxis}
	\end{subfigure}
	}
	\caption{Mean PR AUC test results on the DTU dataset.}
	\label{fig:dtuResultsPR}
\end{figure}
%
\begin{figure}[tb]
	\centerline{
		\includegraphics[width=1.2\textwidth]{img/dtuResults_opponent_legend_cropped.pdf}
	}	
	\centerline{
	\begin{subfigure}[t]{0.6242\textwidth}
		\includegraphics[width=\textwidth]{img/dtuResultsROC_opponent_1.pdf}
		\caption{Arc 1}
		\label{fig:dtuResultsROCarc1}
	\end{subfigure}
	\begin{subfigure}[t]{0.5618\textwidth}
		\includegraphics[width=\textwidth]{img/dtuResultsROC_opponent_2.pdf}
		\caption{Arc 2}
		\label{fig:dtuResultsROCarc2}
	\end{subfigure}
	}
	\centerline{
	\begin{subfigure}[t]{0.6242\textwidth}
		\includegraphics[width=\textwidth]{img/dtuResultsROC_opponent_3.pdf}
		\caption{Arc 3}
		\label{fig:dtuResultsROCarc3}
	\end{subfigure}
	\begin{subfigure}[t]{0.5618\textwidth}
		\includegraphics[width=\textwidth]{img/dtuResultsROC_opponent_4.pdf}
		\caption{Linear path}
		\label{fig:dtuResultsROClinear}
	\end{subfigure}
	}
	\centerline{
	\begin{subfigure}[t]{0.6242\textwidth}
		\includegraphics[width=\textwidth]{img/dtuResultsROC_opponent_5.pdf}
		\caption{Light path, $x$-axis}
		\label{fig:dtuResultsROCyAxis}
	\end{subfigure}
	\begin{subfigure}[t]{0.5618\textwidth}
		\includegraphics[width=\textwidth]{img/dtuResultsROC_opponent_6.pdf}
		\caption{Light path, $z$-axis}
		\label{fig:dtuResultsROCzAxis}
	\end{subfigure}
	}
	\caption{Mean ROC AUC test results on the DTU dataset paths.}
	\label{fig:dtuResultsROC}
\end{figure}
%
\begin{figure}[tb]
	\centerline{
	\begin{subfigure}[t]{0.6242\textwidth}
		\includegraphics[width=\textwidth]{img/dtuResultsStatsGoSi_Sift_1.pdf}
		\caption{Arc 1}
		\label{fig:dtuResultsStatsarc1}
	\end{subfigure}
	\begin{subfigure}[t]{0.5618\textwidth}
		\includegraphics[width=\textwidth]{img/dtuResultsStatsGoSi_Sift_2.pdf}
		\caption{Arc 2}
		\label{fig:dtuResultsStatsarc2}
	\end{subfigure}
	}
	\centerline{
	\begin{subfigure}[t]{0.6242\textwidth}
		\includegraphics[width=\textwidth]{img/dtuResultsStatsGoSi_Sift_3.pdf}
		\caption{Arc 3}
		\label{fig:dtuResultsStatsarc3}
	\end{subfigure}
	\begin{subfigure}[t]{0.5618\textwidth}
		\includegraphics[width=\textwidth]{img/dtuResultsStatsGoSi_Sift_4.pdf}
		\caption{Linear path}
		\label{fig:dtuResultsStatslinear}
	\end{subfigure}
	}
	\centerline{
	\begin{subfigure}[t]{0.6242\textwidth}
		\includegraphics[width=\textwidth]{img/dtuResultsStatsGoSi_Sift_5.pdf}
		\caption{Light path, $x$-axis}
		\label{fig:dtuResultsStatsxAxis}
	\end{subfigure}
	\begin{subfigure}[t]{0.5618\textwidth}
		\includegraphics[width=\textwidth]{img/dtuResultsStatsGoSi_Sift_6.pdf}
		\caption{Light path, $z$-axis}
		\label{fig:dtuResultsStatszAxis}
	\end{subfigure}
	}
	\caption{95\% confidence intervals on the PR AUC difference between our GO+SI descriptor and SIFT. A positive difference denotes that our GO+SI descriptor outperforms SIFT and vice versa. If the confidence interval for a position contains zero, there is no significant difference in performance.}
	\label{fig:dtuResultsStats}
\end{figure}
%
%
\subbibliography
\end{document}
