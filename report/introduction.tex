\documentclass[thesis.tex]{subfiles}
\begin{document}

\chapter{Introduction}
\label{sec:introduction}

In the field of computer vision, researchers have for many years worked on programming computers to interpret real world images as humans do. We focus on two problems within this field: image correspondence \cite{dahl2011finding}, where we search for corresponding points between two different images taken of the same object, and pedestrian detection \cite{felzenszwalb2008discriminatively}, where locations of pedestrians are identified. These objectives are illustrated in \Cref{fig:introduction}. The approaches to these problems are often based on creating descriptors to represent regions of the images, and comparing these descriptors by some method depending on the application. Popular and successful descriptors include SIFT \cite{lowe2004distinctive}, GLOH \cite{mikolajczyk2005performance}, DAISY \cite{tola2008fast}, and HOG \cite{dalal2005histograms,felzenszwalb2009object}.

\begin{figure}[!b]
\centering
	\centerline{\begin{subfigure}[t]{0.51\textwidth}
		\includegraphics[width=\textwidth]{img/introductionIC.pdf}
		\caption{Image correspondence}
		\label{fig:introductionIC}
	\end{subfigure}
	\begin{subfigure}[t]{0.51\textwidth}
		\includegraphics[width=\textwidth]{img/introductionOD.pdf}
		\caption{Pedestrian detection}
		\label{fig:introductionOD}
	\end{subfigure}}
	\caption{Illustrations of the objectives for the two chosen applications. In \subref{fig:introductionIC} we look for corresponding points between two images of the same object. In \subref{fig:introductionOD} we look for pedestrians.}
	\label{fig:introduction}
\end{figure}

The strategy for choosing image regions depends on the application. For pedestrian detection a systematic search across the image is needed, so we use a sliding window approach. For image correspondence we only want to describe the most distinctive regions, and hence we instead use an interest point detector. Commonly used are the Harris corner detector, Hessian based detectors, the Difference of Gaussians (DoG) blob detector, and the Maximally Stable Extremal Regions (MSER) detector \cite{aanaes2012interesting,dahl2011finding}.

An important part of constructing descriptors is to make them invariant to image transformations such as rotation, translation, illumination, scale, perspective, and noise. This causes real world objects to be described similarly despite being captured under different conditions. The choice of invariant properties depends on the application of the descriptor. For example rotation invariance is desired for texture description but not necessarily for recognition of pedestrians.

The overall goal of this project is to study whether higher order information is able to improve descriptor performance. We will study the field of descriptors and based on this create our own, which we compare against state of the art. Like SIFT and HOG, our descriptors are based on derivative scale-space images, which describe differential structure in images at multiple scales. From these we compute histograms of gradient orientation and/or shape index \cite{koenderink1992surface}. The descriptor consists of histograms computed for a grid of cells over a region. To account for local illumination changes, we propose a pixel-wise normalization scheme of the derivative scale-space images.

The report is structured as follows: First we conduct a literature study of descriptors and their applications. Next we describe various methods used within the field of descriptors, which we need throughout the report. Hereafter we propose our descriptor framework, and apply our descriptors to the image correspondence and pedestrian detection problems. Then we discuss the application results, and finally we conclude upon our work.

Throughout this report we move non-essential details of derivations to appendices, and note when this is done.

\subbibliography

\end{document}
