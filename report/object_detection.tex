\documentclass[thesis.tex]{subfiles}
\def\x{\mathbf{x}}
\def\d{\mathbf{d}}
\def\w{\mathbf{w}}
\newcommand\TPR{\mathit{TPR}}
\newcommand\FPR{\mathit{FPR}}
\begin{document}

\chapter{Pedestrian detection}
\label{sec:od}
%
In this chapter we describe the pedestrian detection problem and how we apply our descriptor to solve it.

Object detection is the problem of locating specified objects in images. One application of this is pedestrian detection, which we have decided to focus on as it is simple and has extensive datasets. A common approach to object detection is to extract regions from the image, reducing the problem to binary classification of the presence of the specified object in each region. Using this approach, we are able to compute one descriptor for each region. We furthermore assume fixed size regions in scale-space, allowing us to directly compare our descriptors. The classification of pedestrians is done using a linear support vector machine.

The chapter is structured as follows. First we describe support vector machines, and how to evaluate their classification performance. We then describe the dataset and specify how we apply our descriptor to the problem as well as our experimental setup. Next, we show an example of the use of our descriptor for pedestrian detection and how it relates to HOG, including some example classifications of dataset images. Finally, we present our parameter optimization and test results, which include a comparison with HOG.
%
\section{Support vector machines}
%
Suppose we have some training data split into positive and negative elements. In our case these are $n$-dimensional descriptors of regions with and without pedestrians. We wish to construct a binary classifier which accurately assigns a classification score denoting how confident we are that there is or isn't a pedestrian present. The approach of a linear support vector machine (SVM) is to place a $(n-1)$-dimensional hyperplane in descriptor space which separates the two classes. The hyperplane $(\w,b)$ is defined as the points $\x$ satisfying
%
\begin{align*}
\w \cdot \x - b = 0.
\end{align*}
%
If the data is linearly separable, we can search for the \emph{maximum-margin} hyperplane defined as having the largest possible distance to the nearest point of each class. This distance defines the margin. Formally, this can be written as the constrained optimization problem
%
\begin{align*}
\min_{\w,b} \frac12 \| \w \|^2 \qquad \text{subject to} \qquad y_i (\w \cdot \x_i - b) \geq 1,
\end{align*}
%
where $\x_i$ are training data points and $y_i \in \{-1,1\}$ their respective condition labels.

However, linear separation of the two classes is almost never feasible. Instead a \emph{soft-margin} SVM is used. The idea is to allow some of the data points to be placed on the wrong side of the margin. The total distance of misplaced points to the margin is minimized, in addition to maximizing the margin. This is done by introducing a \emph{cost} parameter $C$ which weights the importance of the misplacement minimization relative to the margin maximization. Formally, the soft-margin optimization problem is written as
%
\begin{align}
\label{eq:svmDefinition}
\min_{\w,\boldsymbol{\xi},b} \left\{ \frac12 \| \w \|^2 + C \sum_i \xi_i \right\} \qquad \text{subject to} \qquad \begin{aligned} y_i (\w \cdot \x_i - b) &\geq 1 - \xi_i, \\ \xi_i &\geq 0, \end{aligned}
\end{align}
%
where $\xi_i$ are slack variables allowing for misplaced points.

If there is a large skew in the number of elements in each class, the larger class will dominate the misplacement term, and we will be less likely to find a good separation. In order to correct this skew, the cost parameter $C$ can be defined for each of the two classes individually. The simplest approach is dividing $C$ by the number of elements in the respective classes.

When using the SVM for solving the pedestrian detection problem, we use the $L_2$-regularized $L_2$ loss support vector classifier from LIBLINEAR v. 1.94\footnote{\url{http://www.csie.ntu.edu.tw/~cjlin/liblinear/}} \cite{fan2008liblinear}. The naming of this SVM type means that the $L_2$-norm is used for computing both $\| \w \|$ and $\xi_i$, as in \Cref{eq:svmDefinition}.

\section{Performance measures}

Having described the method of using a SVM for binary classification, we are now able to define a performance measure for the pedestrian detection problem. Recall that the an SVM gives us the signed distances $\w \cdot \d - b$ between each descriptor $\d$ and the hyperplane dividing the two classes: positive for pedestrians and negative for non-pedestrians. Since the SVM is trained to find the best separation between the classes, we can use this distance as the classification score $s$. Following the binary classification method described in \Cref{sec:binaryClassificationMeasures}, we are able to compute both the PR- and ROC-curves and their AUCs. In other words we vary a threshold on the distance to the hyperplane to get the two measures of the performance of a descriptor.

The dataset we use (described in \Cref{sec:odDataset}) has a large skew in the amount of positives and negatives. This is because the only positives are the exact cut-outs of pedestrians, while negatives can be anything else.
Recall that the PR measure is preferred when there is a large skew towards one of the classes of a binary classification, as described in \Cref{sec:binaryClassificationMeasures}. We will therefore be using the PR measure as performance measure for our pedestrian detection parameter study as well as testing. \Citet{dalal2005histograms} use the log-log plot of the ROC-curve as performance measure, and hence we will show test results for this performance measure as well. In order for this measure to make sense, the actual plot is defined as the logarithm of $1-\text{Recall}$ versus the logarithm of $\FPR$.
%
\section{Dataset}
\label{sec:odDataset}
%
The dataset we use for training and testing our descriptor on the pedestrian detection application is called the \emph{INRIA Person Dataset}\footnote{\url{http://pascal.inrialpes.fr/data/human/}} (from now on called the INRIA dataset) created by \citet{dalal2005histograms}. The purpose of this dataset is to evaluate methods on more challenging images, compared to the earlier MIT Pedestrian dataset \cite{papageorgiou2000trainable}.

It consists of various real world images grouped into two subsets: images with pedestrians (positives) and images without pedestrians (negatives). From the positive images, cut-outs of pedestrians are extracted and rescaled to a fixed size, such that the pedestrian in each cut-out is 96 pixels in height from their feet to shoulders. We extract our own cut-outs from the negative images. The size of the cut-outs are $64 \times 128$ pixels with a 3 pixel border to avoid boundary condition effects. In case a pedestrian cut-out is smaller than the defined size after resizing, the borders are replicated to achieve the desired dimensions.
The positive set only contains images of somewhat upright persons that initially were at least 100 pixels in height. In order to improve the robustness of the dataset against reflected images, each positive cut-out has its horizontally flipped image included as well.

The dataset has been pre-split into training and test data in order to compare methods fairly. The training data has a total of 2416 positive and 1218 negative images, while the test data has a total of 1126 positives and 453 negatives.

\Cref{fig:inriaExampleImages} shows examples of positive \subref{fig:inriaPositives} and negative \subref{fig:inriaNegatives} images from the INRIA dataset. From the positive examples we clearly see the high variety of upright positions and surroundings in the dataset.

%\newgeometry{left=2cm,right=2cm,top=3cm,bottom=3cm}
\begin{figure}[p]
	\centering
	\begin{subfigure}[t]{\textwidth}
		\centerline{\includegraphics[width=1.2\textwidth]{img/inriaPositives.png}}
		\caption{Positives}
		\label{fig:inriaPositives}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{\textwidth}
		\centerline{\includegraphics[width=1.2\textwidth]{img/inriaNegatives.png}}
		\caption{Negatives}
		\label{fig:inriaNegatives}
	\end{subfigure}
	\caption{Example INRIA images.}
	\label{fig:inriaExampleImages}
	%
	\vspace{1cm}
	%
	\centerline{
	\begin{subfigure}[t]{0.593\textwidth}
		\includegraphics[width=\textwidth]{img/inriaManExample1.png}
	\end{subfigure}
	\begin{subfigure}[t]{0.593\textwidth}
		\includegraphics[width=\textwidth]{img/inriaManOccluded.png}
	\end{subfigure}
	}
	\caption{Examples of pedestrians in negative images.}
	\label{fig:inriaNegativePersons}
\end{figure}
%\restoregeometry

\subsection{Pitfalls and deficiencies}
\label{sec:INRIApitfalls}
We have found three minor problems with the INRIA dataset which could affect the results of using the dataset. The first problem is duplicate images. Amongst the negative training images we have found 10 pair-wise duplicates. This could bias the SVM since it effectively means that these images are weighted twice as high as the others. Given the small percentage of duplicates in the dataset the effect should however be close to nothing. We have also found a single duplicated image in the negative test set.

The second problem is the presence of people in some of the negatives.
\Cref{fig:inriaNegativePersons} show two examples of persons in the negative images of the INRIA dataset. The people are however either too small to fit correctly into the sliding window at any of the used scales or partially occluded by the image bounds to such an extent that we shouldn't be able to detect them anyway.

Finally, some of the positives are originally located near image borders. In order to get images of the correct size, the images are padded by repeating the border pixels. This technique generates synthetic gradients tangent to the border. Such gradients could be learned by the SVM and hence influence the final outcome. This could potentially overstate the actual performance of the classifier.
%
\section{Application of descriptors}
\label{sec:inriaDescriptorApplication}
%
As in the image correspondence problem, we will evaluate the performance of the GO, SI, GO+SI and HOG descriptors. We also try combining HOG and SI into a HOG+SI descriptor, to see if adding shape index can improve on HOG. Furthermore we will propose a suboptimal compact GO+SI descriptor, which has lower dimensionality than HOG.
Our descriptors are using the region cell layouts described in \Cref{sec:cellApertureFunctionRegion}, since we are interested in a general region description.

When computing descriptors on images that are larger than the $134 \times 70$ pixel regions (including boundary pixels), we use a sliding window (see \Cref{sec:slidingWindow}) with 10 pixel stride and scales at $2^{\nicefrac{n}{3}},~n=1,\hdots,m$ where $m$ is the largest number such that the entire window is inside the image. 

\citet{dalal2005histograms} showed that smoothing of the pedestrian images resulted in worse performance than without smoothing. We therefore refrain from smoothing the images in our scale-space pyramids. As mentioned in \Cref{sec:hog}, we use RGB images for HOG. During our parameter study we use grayscale images to decrease the amount of computations, whereas we use RGB images for the final classifier.
%
\section{Experimental setup}
\label{sec:inriaExperimentalSetup}
%
We have two experimental setups for evaluating descriptors on the INRIA dataset. The first is a validation setup for evaluating parameters during our parameter study using only the training data, and the second is a more complex test setup for evaluating the optimized or alternative descriptors on test data.

For our validation setup, we split the training data into six parts for the purpose of (leave-one-out) cross-validation. This is done by, for each split, training an SVM on five parts and testing on the other. The SVM is trained using all the given positive images and 40 random cut-outs from each negative image. The results are consolidated by computing the mean PR AUC across the six splits.

Our test setup is a modification of the one described by \citet{dalal2005histograms}. First, an SVM is trained using all positive training images and 40 random cut-outs from each negative training image. A sliding window is then run across these negative training images, and these windows are classified by the SVM in order to find hard negatives (false positives). The hard negatives are defined as the negative cut-outs with classification score $s > 0$. In other words, we search the negative training images for examples that have not been learned by the SVM from the initial random cut-outs. We add the hard negatives to the set of existing negatives and retrain an SVM with the new training set. This SVM is then used for testing on the positive test images and sliding windows across negative test images.

Compared to \citet{dalal2005histograms}, we extract a lot more cut-outs (40 per image as opposed to 10) and add a lot more hard negatives ($10^5$ as opposed to just the positive classification scores). The old amounts of training examples were caused by hardware limitations of only having $1.7$ GB of RAM for SVM training. We also prefer using a fixed number of hard negatives in order to train the SVM on the same number of images for each descriptor.

\section{Example}
%
In this section we will show an example of our proposed GO descriptor calculated from a positive pedestrian image. Furthermore we illustrate the connection between the GO descriptor and the weights of a SVM classifier trained using the test setup in \Cref{sec:inriaExperimentalSetup}.
Finally we will show examples of the positive and negative images whose SVM classification we have most and least confidence in.

\begin{figure}[tb]
	\centerline{
	\begin{subfigure}[t]{0.17\textwidth}
		\includegraphics[width=\textwidth]{img/hogExample1.jpg}
		\caption{}
		\label{fig:hogExample1}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{0.17\textwidth}
		\includegraphics[width=\textwidth]{img/hogExample2.png}
		\caption{}
		\label{fig:hogExample2}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{0.17\textwidth}
		\includegraphics[width=\textwidth]{img/hogExample3.png}
		\caption{}
		\label{fig:hogExample3}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{0.17\textwidth}
		\includegraphics[width=\textwidth]{img/hogExample4.jpg}
		\caption{}
		\label{fig:hogExample4}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{0.17\textwidth}
		\includegraphics[width=\textwidth]{img/hogExample5.png}
		\caption{}
		\label{fig:hogExample5}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{0.17\textwidth}
		\includegraphics[width=\textwidth]{img/hogExample6.png}
		\caption{}
		\label{fig:hogExample6}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{0.17\textwidth}
		\includegraphics[width=\textwidth]{img/hogExample7.png}
		\caption{}
		\label{fig:hogExample7}
		\vspace{2mm}
	\end{subfigure}}
	\caption{Example of the HOG descriptor taken from \cite[Figure 6]{dalal2005histograms}. \subref{fig:hogExample1} shows the average gradient image over training data. \subref{fig:hogExample2} and \subref{fig:hogExample3} show the maximal positive and minimal negative SVM weights for each block centered at the pixels, respectively. \subref{fig:hogExample4} shows a test image. \subref{fig:hogExample5} shows its computed descriptor. \subref{fig:hogExample6} and \subref{fig:hogExample7} show the descriptor weighted by positive and negative SVM weights, respectively.}
	\label{fig:hogExample}
%	\stepcounter{figure}
	\vspace*{\floatsep}
	\setcounter{subfigure}{0}
	\centering
	\centerline{
	\begin{subfigure}[t]{0.1635\textwidth}
		\includegraphics[width=\textwidth]{img/inriaPosTrainMeanM.png}
		\caption{}
		\label{fig:inriaPosTrainMeanM}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{0.17\textwidth}
		\includegraphics[width=\textwidth]{img/inriaExampleDescriptorSvmMax.pdf}
		\caption{}
		\label{fig:inriaExampleDescriptorSvmMax}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{0.17\textwidth}
		\includegraphics[width=\textwidth]{img/inriaExampleDescriptorSvmMin.pdf}
		\caption{}
		\label{fig:inriaExampleDescriptorSvmMin}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{0.17\textwidth}
		\includegraphics[width=\textwidth]{img/inriaExampleCells.pdf}
		\caption{}
		\label{fig:inriaExampleCells}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{0.17\textwidth}
		\includegraphics[width=\textwidth]{img/inriaExampleDescriptor.pdf}
		\caption{}
		\label{fig:inriaExampleDescriptor}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{0.17\textwidth}
		\includegraphics[width=\textwidth]{img/inriaExampleDescriptorSvm.pdf}
		\caption{}
		\label{fig:inriaExampleDescriptorSvm}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{0.17\textwidth}
		\includegraphics[width=\textwidth]{img/inriaExampleDescriptorSvmNeg.pdf}
		\caption{}
		\label{fig:inriaExampleDescriptorSvmNeg}
		\vspace{2mm}
	\end{subfigure}}
	\caption{Example of our GO descriptor following the same procedure as \Cref{fig:hogExample}. \subref{fig:inriaExampleDescriptorSvmMax} and \subref{fig:inriaExampleDescriptorSvmMin} are however shown for each cell instead of the HOG blocks, and upon \subref{fig:inriaExampleCells} we have illustrated out grid layout using the cell spacing $r$.}
	\label{fig:inriaExample}
\end{figure}
%
\begin{figure}[p]
	\begin{subfigure}[t]{\textwidth}
		\centerline{\includegraphics[width=1.2\textwidth]{img/objectDetectionTP.png}}
		\caption{True positives}
		\label{fig:objectDetectionTP}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{\textwidth}
		\centerline{\includegraphics[width=1.2\textwidth]{img/objectDetectionFN.png}}
		\caption{False negatives}
		\label{fig:objectDetectionFN}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{\textwidth}
		\centerline{\includegraphics[width=1.2\textwidth]{img/objectDetectionTN.png}}
		\caption{True negatives}
		\label{fig:objectDetectionTN}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{\textwidth}
		\centerline{\includegraphics[width=1.2\textwidth]{img/objectDetectionFP.png}}
		\caption{False positives}
		\label{fig:objectDetectionFP}
	\end{subfigure}
	\caption{Example classifications of INRIA cut-outs by our GO descriptor. This shows the types of images our trained SVM is best and worst at classifying.}
	\label{fig:ownDescriptorExample}
\end{figure}
%
\Cref{fig:hogExample} shows a descriptor example from \citet{dalal2005histograms}, and \Cref{fig:inriaExample} shows the corresponding images reproduced for our GO descriptor. Since HOG has each histogram of gradients ``repeated'' by using block normalization, only the blocks centered at each pixel are illustrated. Our descriptor is illustrated for each cell of the descriptor. The average gradient images over the training data in \subref{fig:inriaPosTrainMeanM} are similar to one another, as they both are computed using central difference filters without smoothing. Likewise we clearly see the silhouette of a pedestrian in these average images.

The maximal positive and minimal negative SVM weights in \subref{fig:inriaExampleDescriptorSvmMax} and \subref{fig:inriaExampleDescriptorSvmMin} differ quite a lot since our descriptor has a much more dense grid. There are however some overall similarities such as large weights around the head and foot regions of the positive weights, and low weights around the foot regions of the negative SVM weights. In other words structure around the head and foot regions count towards a pedestrian detection.

Having described the SVM weights, we now look at the descriptors in \subref{fig:inriaExampleDescriptor} computed from the images in \subref{fig:inriaExampleCells}. The descriptors weighted by the positive and negative SVM weights are illustrated in \subref{fig:inriaExampleDescriptorSvm} and \subref{fig:inriaExampleDescriptorSvmNeg} respectively. Both HOG and our descriptor visualize the head, shoulders, and foot regions of the pedestrian in \subref{fig:inriaExampleDescriptorSvm}. From \subref{fig:inriaExampleDescriptorSvmNeg} we see that vertical lines inside the pedestrian count against a pedestrian detection for HOG, whereas there is little to conclude for our descriptor. As mentioned in \Cref{sec:INRIApitfalls} repeated boundaries could be learned by the SVM. In the visualized descriptor (bottom row) we easily see the repeated border in the bottom of the image, and a couple of these contribute quite a lot to the positive SVM.

To get an idea of the types of images our trained SVM is best and worst at classifying, we find some of the pedestrian and non-pedestrian images with the highest and lowest classification scores. We call the pedestrian images with high classification scores true positives, as we are more likely to classify them correctly. Whether we do so of course depends on the exact threshold chosen. Similar statements can be said for false positives, true negatives, and false negatives. The images are shown in \Cref{fig:ownDescriptorExample}.

Generally the true positive images show pedestrians in standard up-right poses with the front or back to the camera. The clothing is clearly distinctive from the background, and there are little to no occlusions. Furthermore 6 of the 10 true positives illustrated here have a repeated bottom boundary. The false negatives are quite the opposite: many occlusions, people facing sideways, and some dim lighting. The true negative examples are all sky, with very little gradient structure. The false positives are quite varied; some have structure around the possible foot or head region, but it is not obvious to see how they could be classified as pedestrians.
%
\section{Parameter study}
\label{sec:odParameterStudy}
%
%We wish to optimize the descriptor parameters listed in \Cref{sec:descriptorParameters} with the objective of maximizing average matching PR AUC. We do this separately for GO and SI as they may have different optimal parameters, and simply combine these to create the GO+SI descriptor. As mentioned in the experimental setup, we have split the DTU dataset into six parts to reduce overfitting by cross-validation. For each iteration we optimize parameters on five parts and test the optimized descriptor on the final part. This means we obtain six sets of parameters and a single PR AUC for each image, where the images are evaluated without optimizing across their respective scenes. To save time evaluating parameters, we scale the images to half size and convert them to grayscale. The testing is done on the full scale images converted to opponent colour space as mentioned in \Cref{sec:opponentColourSpace}.
%
%Our actual parameter optimization is problematic because we need to optimize ten parameters at once, and each evaluation is computationally heavy. Our basic strategy is thus to cycle through each parameter and optimize it separately while fixing the other parameters. We do this a number of iterations with the hope of reaching convergence.
%The risk of this strategy is getting stuck in local optima. To avoid this, recall that we have defined the parameters to reduce correlation: e.g. when bin count $n$ is increased, the width of the bins is automatically reduced to compensate. We also select certain correlated parameters to optimize together: grid type with grid size, as well as cell and bin kernels with their respective scales. Furthermore we optimize continuous parameters in two steps: first a coarse search and then a finer search around the best result.
%
%The optimal parameters of our parameter study for our GO and SI descriptors are shown in \Cref{fig:ICparamsGo,fig:ICparamsSi} respectively.

We follow the same setup as our image correspondence parameter study in \Cref{sec:icParameterStudy}, with a couple exceptions. Since the dataset has a pre-defined split into training and test data, we only run the parameter study once (with 6-fold cross validation) on training data and obtain a single set of parameters. The parameters are also slightly different of the region cell layout and the addition of the SVM-weight $C$, which we also wish to optimize for each descriptor.

The optimal parameters for our GO and SI descriptors are shown in \Cref{tbl:INRIAparams}, together with the parameters for HOG translated into our framework. The optimal $C$ values for all descriptors are shown in \Cref{tbl:INRIAparamC}. The effect of adjusting the parameters relative to the optimized parameters can be seen for GO in \Cref{fig:inriaParametersGoAuc} and SI in \Cref{fig:inriaParametersSiAuc}. Cell spacing $r$ and bin count $n$ seem to the two most important parameters: Decreasing $r$ improves performance greatly, but the increased number of cells also results in a much higher dimensionality. For GO $n$ seems to produce better results for high odd numbers than the neighbouring even numbers, whereas 9 or 13 bins produce good results for SI. The optimized $r$ values are the lowest in our search range for each grid type, because the dimensionality at this point is already almost too high to work with. Most of the other parameters have a large range of values that produce good results. Box filters marginally outperform the alternative kernels, except for the bin kernel of SI, where the three kernels perform almost equally well, and the triangle kernel is chosen.

Due to the high dimensionality we construct a compact version of our GO+SI descriptor based on these training results. We keep the same optimal parameters, except we increase cell spacing $r = 2.54$ in a square grid, and reduce bin count to $n = 13$ for GO and $n = 5$ for SI. This reduces the dimensionality below HOG. For our HOG+SI descriptor, we reduce $r = 2.18$ and $n = 9$ to reduce the influence of the added shape index information compared to HOG. We will go into further detail about the descriptor dimensionality and performance in the results section.

\begin{table}[tb]
\centering
\begin{tabular}{ l c c c }
\toprule
{} & \multicolumn{3}{c}{Methods} \\
Parameter & GO & SI & HOG \\ \midrule
Grid type & Square & Triangle & Square \\
Cell spacing $r$ & $1.57$ & $1.62$ & $4$ \\
Cell kernel & \textit{Box} & \textit{Box} & \textit{Tri} \\
Cell scale $\alpha$ & $3.1$ & $3.5$ & $1.0$ \\
Bin count $n$ & $15$ & $13$ & $9+18$ \\
Bin kernel & \textit{Box} & \textit{Tri} & \textit{Tri} \\
Bin scale $\beta$ & $1.4$ & $0.8$ & $1.0$ \\
Normalization scale $\eta$ & $4.0$ & $5.0$ & - \\
\bottomrule
\end{tabular}
\caption{Optimized parameters from our parameter study for GO and SI. We also include the HOG parameters translated into our framework when possible. HOG has 9 undirected and 18 directed orientation bins, and uses a block normalization scheme instead of our pixel normalization.}
\label{tbl:INRIAparams}
\end{table}
%
\begin{table}[tb]
\centerline{
\begin{tabular}{ l c c c c c c }
\toprule
{} & \multicolumn{4}{c}{Methods} \\
Parameter & GO & SI & GO+SI & Compact GO+SI & HOG & HOG+SI \\ \midrule
$\log C$ & $6.3\cdot 10^{-4}$ & $1.3\cdot 10^{-4}$ & $6.3\cdot 10^{-4}$ & $4.0\cdot 10^{-3}$ & $2.0\cdot 10^{-3}$ & $3.2\cdot 10^{-3}$ \\
\bottomrule
\end{tabular}}
\caption{Optimized SVM weight C for GO, SI, GO+SI, Compact GO+SI, HOG and HOG+SI.}
\label{tbl:INRIAparamC}
\end{table}
%
\begin{figure}[p]
\centering
	\centerline{\begin{subfigure}[t]{0.593\textwidth}
		\includegraphics[width=\textwidth]{img/inriaParametersGo_cellSpacing.pdf}
		\caption{Cell spacing $r$}
		\label{fig:inriaParametersGo_cellSpacing}
	\end{subfigure}
	\begin{subfigure}[t]{0.593\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/inriaParametersGo_binCount.pdf}
		\caption{Bin count $n$}
		\label{fig:inriaParametersGo_binCount}
	\end{subfigure}}
	\centerline{\begin{subfigure}[t]{0.593\textwidth}
		\includegraphics[width=\textwidth]{img/inriaParametersGo_cellSigmaAlt.pdf}
		\caption{Cell scale $\alpha$}
		\label{fig:inriaParametersGo_cellSigma}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{0.593\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/inriaParametersGo_binSigmaAlt.pdf}
		\caption{Bin scale $\beta$}
		\label{fig:inriaParametersGo_binSigma}
	\end{subfigure}}
	\centerline{\begin{subfigure}[t]{0.593\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/inriaParametersGo_normSigma.pdf}
		\caption{Normalization scale $\eta$}
		\label{fig:inriaParametersGo_normSigma}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{0.593\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/inriaParametersGo_logC.pdf}
		\caption{SVM weight $C$}
		\label{fig:inriaParametersGo_logC}
		\vspace{2mm}
	\end{subfigure}}
	\caption{Average classification performance on training data for GO when adjusting the parameters. Optimal parameter values are marked with a cross.}
	\label{fig:inriaParametersGoAuc}
	\vspace{1cm}
	\centerline{\begin{subfigure}[t]{0.593\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/inriaParametersGo_cellSpacingDims.pdf}
		\caption{Cell spacing $r$}
		\label{fig:inriaParametersGo_cellSpacingDims}
	\end{subfigure}
	\begin{subfigure}[t]{0.593\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/inriaParametersGo_binCountDims.pdf}
		\caption{Bin count $n$}
		\label{fig:inriaParametersGo_binCountDims}
		\vspace{2mm}
	\end{subfigure}}
	\caption{Dimensionality of GO when adjusting the parameters.}
	\label{fig:inriaParametersGoDims}
\end{figure}
%
\begin{figure}[p]
\centering
	\centerline{\begin{subfigure}[t]{0.593\textwidth}
		\includegraphics[width=\textwidth]{img/inriaParametersSi_cellSpacing.pdf}
		\caption{Cell spacing $r$}
		\label{fig:inriaParametersSi_cellSpacing}
	\end{subfigure}
	\begin{subfigure}[t]{0.593\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/inriaParametersSi_binCount.pdf}
		\caption{Bin count $n$}
		\label{fig:inriaParametersSi_binCount}
	\end{subfigure}}
	\centerline{\begin{subfigure}[t]{0.593\textwidth}
		\includegraphics[width=\textwidth]{img/inriaParametersSi_cellSigmaAlt.pdf}
		\caption{Cell scale $\alpha$}
		\label{fig:inriaParametersSi_cellSigma}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{0.593\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/inriaParametersSi_binSigmaAlt.pdf}
		\caption{Bin scale $\beta$}
		\label{fig:inriaParametersSi_binSigma}
	\end{subfigure}}
	\centerline{\begin{subfigure}[t]{0.593\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/inriaParametersSi_normSigma.pdf}
		\caption{Normalization scale $\eta$}
		\label{fig:inriaParametersSi_normSigma}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{0.593\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/inriaParametersSi_logC.pdf}
		\caption{SVM weight $C$}
		\label{fig:inriaParametersSi_logC}
		\vspace{2mm}
	\end{subfigure}}
	\caption{Average classification performance on training data for SI when adjusting the parameters. Optimal parameter values are marked with a cross.}
	\label{fig:inriaParametersSiAuc}
	\vspace{1cm}
	\centerline{\begin{subfigure}[t]{0.593\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/inriaParametersSi_cellSpacingDims.pdf}
		\caption{Cell spacing $r$}
		\label{fig:inriaParametersSi_cellSpacingDims}
	\end{subfigure}
	\begin{subfigure}[t]{0.593\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/inriaParametersSi_binCountDims.pdf}
		\caption{Bin count $n$}
		\label{fig:inriaParametersSi_binCountDims}
		\vspace{2mm}
	\end{subfigure}}
	\caption{Dimensionality of SI when adjusting the parameters.}
	\label{fig:inriaParametersSiDims}
\end{figure}
%
We have also experimented with various other parts of our descriptor that are not listed as parameters. We report some of these results for our optimal GO descriptor on training data, which has a mean PR AUC of $0.972$: Normalizing the final descriptors before training the SVMs reduces the AUC by $0.539$. Computing the gradients on RGB colour channels and taking the largest magnitude improves the AUC by $0.003$. Omitting pixel normalization reduces the AUC by $0.066$. Smoothing images at scale $\sigma = 1$ reduces the AUC by $0.015$. These tendencies hold for SI as well.
%
\section{Results}
%
Now that we have computed optimal parameters on the training data, we can evaluate the performance of the descriptors on test data. As mentioned before, we compare our descriptor to HOG, which originally was trained and tested on the same INRIA dataset \cite{dalal2005histograms}. Recall that we use RGB images for testing.

\Cref{tbl:inriaResults} shows a test performance summary for the six descriptors, their dimensionality and amount of hard negatives added for retraining. We also include the test performance (initial PR AUC) when omitting retraining to see the effect of this step. Based on PR AUC the ordering of the descriptors from best to worst is: HOG+SI, HOG, compact GO+SI, GO+SI, GO, SI. The same ordering holds for recall at $10^{-4}$ FPR, which is the measure used by \citet{dalal2005histograms}, whereas HOG and compact GO+SI switch positions when looking at ROC AUC.

Overall HOG outperforms our descriptors, but the addition of shape index is shown to be valuable for both HOG and our descriptors. The SI descriptor performs very poorly on its own. The difference between compact GO+SI and GO+SI is negligible despite the large difference in dimensionality.

By comparing the PR AUCs with and without retraining of hard negatives, we see that all descriptors benefit from retraining. Interestingly compact GO+SI starts off worse than the much larger GO+SI, but overtakes it after retraining by adding more than twice the amount of hard negatives.

\Cref{fig:inriaTestResults} shows ROC- and PR-curves of the various descriptors. Both curves show roughly the same ordering of descriptor performances as the PR AUCs. However the compact GO+SI, GO+SI, and GO curves cross each other multiple times, and hence their ordering depends on the chosen threshold $t$. The HOG results shown here are not quite as good as those presented by \citet{dalal2005histograms}. This may be due to the VLFeat implementation of HOG, SVM differences, or our sliding window implementation.

\Cref{fig:inriaHistogram} illustrates the distribution of classification scores for positive and negative images. Each class is normalized with respect to the amount of images in the class. The graphs give a more intuitive understanding of the performance of the descriptors. The amount of overlap between the two classes illustrates the separability of each classifier and the effect of choosing a threshold $t$.

\begin{table}[tb]
\centerline{
\begin{tabular}{ l c c c c c c }
\toprule
Measure & HOG+SI & HOG & Compact GO+SI & GO+SI & GO & SI \\ \midrule
Dimensions & 8316 & 4743 & 4554 & 21346 & 11115 & 10231 \\
Hard negatives & 10703 & 24387 & 28528 & 12705 & 24547 & 75033 \\
Initial PR AUC & 0.922 & 0.903 & 0.839 & 0.877 & 0.833 & 0.608 \\
PR AUC & 0.951 & 0.919 & 0.902 & 0.891 & 0.881 & 0.680 \\
ROC AUC & 0.9990 & 0.9980 & 0.9983 & 0.9973 & 0.9971 & 0.9888 \\
Recall at $10^{-4}$ FPR & 0.910 & 0.850 & 0.824 & 0.811 & 0.804 & 0.511 \\
\bottomrule
\end{tabular}}
\caption{Summary of test results on the INRIA dataset. The descriptors are ordered from best to worst. For each descriptor we report the dimensionality, number of hard negatives added to the training data, test PR AUC without added hard negatives, and test PR AUC, ROC AUC, and recall at $10^{-4}$ FPR after the hard negatives are added. The latter measure is the one used by \Citet{dalal2005histograms}.}
\label{tbl:inriaResults}
\end{table}
%
\begin{figure}[tb]
\centering
	\centerline{\includegraphics[width=1.2\textwidth]{img/inriaTestResultsLegend.pdf}}
	\centerline{\begin{subfigure}[t]{0.593\textwidth}
		\includegraphics[width=\textwidth]{img/inriaTestResultsROC.pdf}
		\caption{Log-log ROC-curve}
		\label{fig:inriaTestResultsROC}
	\end{subfigure}
	\begin{subfigure}[t]{0.593\textwidth}
		\includegraphics[width=\textwidth]{img/inriaTestResultsPR.pdf}
		\caption{PR-curve}
		\label{fig:inriaTestResultsPR}
	\end{subfigure}}
	\caption{ROC- and PR-curve of the test results on the INRIA dataset. The ROC-curve is plotted in the same range as \citet{dalal2005histograms}, and the PR-curve is plotted starting from recall 0.5 for better visualization.}
	\label{fig:inriaTestResults}
\end{figure}
%
\begin{figure}
	\centering
	\begin{subfigure}[t]{\textwidth}
		\includegraphics[width=\textwidth]{img/inriaHistogramHogSiFinal.pdf}
		\caption{HOG+SI}
		\label{fig:inriaHistogramHogSiFinal}
	\end{subfigure}
	\begin{subfigure}[t]{\textwidth}
		\includegraphics[width=\textwidth]{img/inriaHistogramHogFinal.pdf}
		\caption{HOG}
		\label{fig:inriaHistogramHogFinal}
	\end{subfigure}
	\begin{subfigure}[t]{\textwidth}
		\includegraphics[width=\textwidth]{img/inriaHistogramCompact3Final.pdf}
		\caption{Compact GO+SI}
		\label{fig:inriaHistogramCompact3Final}
	\end{subfigure}
	\begin{subfigure}[t]{\textwidth}
		\includegraphics[width=\textwidth]{img/inriaHistogramGoSiFinal.pdf}
		\caption{GO+SI}
		\label{fig:inriaHistogramGoSiFinal}
	\end{subfigure}
	\begin{subfigure}[t]{\textwidth}
		\includegraphics[width=\textwidth]{img/inriaHistogramGoFinal.pdf}
		\caption{GO}
		\label{fig:inriaHistogramGoFinal}
	\end{subfigure}
	\begin{subfigure}[t]{\textwidth}
		\includegraphics[width=\textwidth]{img/inriaHistogramSiFinal.pdf}
		\caption{SI}
		\label{fig:inriaHistogramSiFinal}
	\end{subfigure}
	\caption{Histograms of classification scores on INRIA test data for various descriptors. The negative and positive results are individually normalized due to the large skew in amount of images.}
	\label{fig:inriaHistogram}
\end{figure}
%
\subbibliography

\end{document}
