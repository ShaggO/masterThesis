\documentclass[thesis.tex]{subfiles}
\def\x{\mathbf{x}}
\def\d{\mathbf{d}}
\def\w{\mathbf{w}}
\newcommand\TPR{\mathit{TPR}}
\newcommand\FPR{\mathit{FPR}}
\begin{document}

\chapter{Pedestrian detection}
\label{sec:od}
%
In this chapter we describe the pedestrian detection problem and how we apply our descriptor to solve it.

Object detection is the problem of locating specified objects in images. One application of this is pedestrian detection, which we have decided to focus on as it is simple and has extensive datasets. A common approach to object detection is to extract regions from the image, reducing the problem to binary classification of the presence of the specified object in each region. Using this approach, we are able to compute one descriptor for each region. We furthermore assume fixed size regions in scale-space, allowing us to directly compare our descriptors. The classification of pedestrians is done using a linear support vector machine.

The chapter is structured as follows. First we describe support vector machines, and how to evaluate their classification performance. We then describe the dataset and specify how we apply our descriptor to the problem as well as our experimental setup. Next, we show an example of the use of our descriptor for pedestrian detection and how it relates to HOG, including some example classifications of dataset images. Finally, we present our parameter optimization and test results, which include a comparison with HOG.
%
\section{Support vector machines}
%
Suppose we have some training data split into positive and negative elements. In our case these are $n$-dimensional descriptors of regions with and without pedestrians. We wish to construct a binary classifier which accurately assigns a classification score denoting how confident we are that there is or isn't a pedestrian present. The approach of a linear support vector machine (SVM) is to place a $(n-1)$-dimensional hyperplane in descriptor space which separates the two classes. The hyperplane $(\w,b)$ is defined as the points $\x$ satisfying
%
\begin{align*}
\w \cdot \x - b = 0.
\end{align*}
%
If the data is linearly separable, we can search for the \emph{maximum-margin} hyperplane defined as having the largest possible distance to the nearest point of each class. This distance defines the margin. Formally, this can be written as the constrained optimization problem
%
\begin{align*}
\min_{\w,b} \frac12 \| \w \|^2 \qquad \text{subject to} \qquad y_i (\w \cdot \x_i - b) \geq 1,
\end{align*}
%
where $\x_i$ are training data points and $y_i \in \{-1,1\}$ their respective condition labels.

However, linear separation of the two classes is almost never feasible. Instead a \emph{soft-margin} SVM is used. The idea is to allow some of the data points to be placed on the wrong side of the margin. The total distance of misplaced points to the margin is minimized, in addition to maximizing the margin. This is done by introducing a \emph{cost} parameter $C$ which weights the importance of the misplacement minimization relative to the margin maximization. Formally, the soft-margin optimization problem is written as
%
\begin{align}
\label{eq:svmDefinition}
\min_{\w,\boldsymbol{\xi},b} \left\{ \frac12 \| \w \|^2 + C \sum_i \xi_i \right\} \qquad \text{subject to} \qquad \begin{aligned} y_i (\w \cdot \x_i - b) &\geq 1 - \xi_i, \\ \xi_i &\geq 0, \end{aligned}
\end{align}
%
where $\xi_i$ are slack variables allowing for misplaced points.

If there is a large skew in the number of elements in each class, the larger class will dominate the misplacement term, and we will be less likely to find a good separation. In order to correct this skew, the cost parameter $C$ can be defined for each of the two classes individually. The simplest approach is dividing $C$ by the number of elements in the respective classes.

When using the SVM for solving the pedestrian detection problem, we use the $L_2$-regularized $L_2$ loss support vector classifier from LIBLINEAR v. 1.94\footnote{\url{http://www.csie.ntu.edu.tw/~cjlin/liblinear/}} \cite{fan2008liblinear}. The naming of this SVM type means that the $L_2$-norm is used for computing both $\| \w \|$ and $\xi_i$, as in \Cref{eq:svmDefinition}.

\section{Performance measures}

Having described the method of using a SVM for binary classification, we are now able to define a performance measure for the pedestrian detection problem. Recall that the an SVM gives us the signed distances $\w \cdot \d - b$ between each descriptor $\d$ and the hyperplane dividing the two classes: positive for pedestrians and negative for non-pedestrians. Since the SVM is trained to find the best separation between the classes, we can use this distance as the classification score $s$. Following the binary classification method described in \Cref{sec:binaryClassificationMeasures}, we are able to compute both the PR- and ROC-curves and their AUCs. In other words we vary a threshold on the distance to the hyperplane to get the two measures of the performance of a descriptor.

The dataset we use (described in \Cref{sec:odDataset}) has a large skew in the amount of positives and negatives. This is because the only positives are the exact cut-outs of pedestrians, while negatives can be anything else.
Recall that the PR measure is preferred when there is a large skew towards one of the classes of a binary classification, as described in \Cref{sec:binaryClassificationMeasures}. We will therefore be using the PR measure as performance measure for our pedestrian detection parameter study as well as testing. \Citet{dalal2005histograms} use the log-log plot of the ROC-curve as performance measure, and hence we will show test results for this performance measure as well. In order for this measure to make sense, the actual plot is defined as the logarithm of $1-\text{Recall}$ versus the logarithm of $\FPR$.
%
\section{Dataset}
\label{sec:odDataset}
%
The dataset we use for training and testing our descriptor on the pedestrian detection application is called the \emph{INRIA Person Dataset}\footnote{\url{http://pascal.inrialpes.fr/data/human/}} (from now on called the INRIA dataset) created by \citet{dalal2005histograms}. The purpose of this dataset is to evaluate methods on more challenging images, compared to the earlier MIT Pedestrian dataset \cite{papageorgiou2000trainable}.

It consists of various real world images grouped into two subsets: images with pedestrians (positives) and images without pedestrians (negatives). From the positive images, cut-outs of pedestrians are extracted and rescaled to a fixed size, such that the pedestrian in each cut-out is 96 pixels in height from their feet to shoulders. We extract our own cut-outs from the negative images. The size of the cut-outs are $64 \times 128$ pixels with a 3 pixel border to avoid boundary condition effects. In case a pedestrian cut-out is smaller than the defined size after resizing, the borders are replicated to achieve the desired dimensions.
The positive set only contains images of somewhat upright persons that initially were at least 100 pixels in height. In order to improve the robustness of the dataset against reflected images, each positive cut-out has its horizontally flipped image included as well.

The dataset has been pre-split into training and test data in order to compare methods fairly. The training data has a total of 2416 positive and 1218 negative images, while the test data has a total of 1126 positives and 453 negatives.

\Cref{fig:inriaExampleImages} shows examples of positive \subref{fig:inriaPositives} and negative \subref{fig:inriaNegatives} images from the INRIA dataset. From the positive examples we clearly see the high variety of upright positions and surroundings in the dataset.

%\newgeometry{left=2cm,right=2cm,top=3cm,bottom=3cm}
\begin{figure}[p]
	\centering
	\begin{subfigure}[t]{\textwidth}
		\centerline{\includegraphics[width=1.2\textwidth]{img/inriaPositives.png}}
		\caption{Positives}
		\label{fig:inriaPositives}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{\textwidth}
		\centerline{\includegraphics[width=1.2\textwidth]{img/inriaNegatives.png}}
		\caption{Negatives}
		\label{fig:inriaNegatives}
	\end{subfigure}
	\caption{Example INRIA images.}
	\label{fig:inriaExampleImages}
	%
	\vspace{1cm}
	%
	\centerline{
	\begin{subfigure}[t]{0.593\textwidth}
		\includegraphics[width=\textwidth]{img/inriaManExample1.png}
	\end{subfigure}
	\begin{subfigure}[t]{0.593\textwidth}
		\includegraphics[width=\textwidth]{img/inriaManOccluded.png}
	\end{subfigure}
	}
	\caption{Examples of pedestrians in negative images.}
	\label{fig:inriaNegativePersons}
\end{figure}
%\restoregeometry

\subsection{Pitfalls and deficiencies}
\label{sec:INRIApitfalls}
We have found three minor problems with the INRIA dataset which could affect the results of using the dataset. The first problem is duplicate images. Amongst the negative training images we have found 10 pair-wise duplicates. This could bias the SVM since it effectively means that these images are weighted twice as high as the others. Given the small percentage of duplicates in the dataset the effect should however be close to nothing. We have also found a single duplicated image in the negative test set.

The second problem is the presence of people in some of the negatives.
\Cref{fig:inriaNegativePersons} show two examples of persons in the negative images of the INRIA dataset. The people are however either too small to fit correctly into the sliding window at any of the used scales or partially occluded by the image bounds to such an extent that we shouldn't be able to detect them anyway.

Finally, some of the positives are originally located near image borders. In order to get images of the correct size, the images are padded by repeating the border pixels. This technique generates synthetic gradients tangent to the border. Such gradients could be learned by the SVM and hence influence the final outcome. This could potentially overstate the actual performance of the classifier.
%
\section{Application of descriptors}
\label{sec:inriaDescriptorApplication}
%
As in the image correspondence problem, we will evaluate the performance of the GO, SI, GO+SI and HOG descriptors. Furthermore we will propose a suboptimal compact GO descriptor, which has lower dimensionality than HOG.
Our descriptors are using the region cell layouts described in \Cref{sec:cellApertureFunctionRegion}, since we are interested in a general region description.

When computing descriptors on images that are larger than the $134 \times 70$ pixel regions (including boundary pixels), we use a sliding window (see \Cref{sec:slidingWindow}) with 10 pixel stride and scales at $2^{\nicefrac{n}{3}},~n=1,\hdots,m$ where $m$ is the largest number such that the entire window is inside the image. 

\citet{dalal2005histograms} showed that smoothing of the pedestrian images resulted in worse performance than without smoothing. We therefore refrain from smoothing the images in our scale-space pyramids. As mentioned in \Cref{sec:hog}, we use RGB images for HOG. During our parameter study we use grayscale images to decrease the amount of computations, whereas we use RGB images for the final classifier.
%
\section{Experimental setup}
\label{sec:inriaExperimentalSetup}
%
We have two experimental setups for evaluating descriptors on the INRIA dataset. The first is a validation setup for evaluating parameters during our parameter study using only the training data, and the second is a more complex test setup for evaluating the optimized or alternative descriptors on test data.

For our validation setup, we split the training data into six parts for the purpose of (leave-one-out) cross-validation. This is done by, for each split, training an SVM on five parts and testing on the other. The SVM is trained using all the given positive images and 40 random cut-outs from each negative image. The results are consolidated by computing the mean PR AUC across the six splits.

Our test setup is a modification of the one described by \citet{dalal2005histograms}. First, an SVM is trained using all positive training images and 40 random cut-outs from each negative training image. A sliding window is then run across these negative training images, and these windows are classified by the SVM in order to find false positives. In other words, we search the negative training images for examples that have not been learned by the SVM from the initial random cut-outs. These images are called \emph{hard negatives}. We pick a fixed number of hard negatives by finding the images with the $10^5$ largest classification scores (out of $2.2 \cdot 10^6$ total). These are the negative images our SVM is most uncertain about. We retrain an SVM with the added hard negatives, and test on the positive test images and sliding windows across negative test images.

Compared to \citet{dalal2005histograms}, we extract a lot more cut-outs (40 per image as opposed to 10) and add a lot more hard negatives ($10^5$ as opposed to just the positive classification scores). The old amounts of training examples were caused by hardware limitations of only having $1.7$ GB of RAM for SVM training. We also prefer using a fixed number of hard negatives in order to train the SVM on the same number of images for each descriptor.

\section{Example}
%
In this section we will show an example of our proposed GO descriptor calculated from a positive pedestrian image. Furthermore we illustrate the connection between the GO descriptor and the weights of a SVM classifier trained using the test setup in \Cref{sec:inriaExperimentalSetup}.
Finally we will show examples of the positive and negative images whose SVM classification we have most and least confidence in.

\begin{figure}[tb]
	\centerline{
	\begin{subfigure}[t]{0.17\textwidth}
		\includegraphics[width=\textwidth]{img/hogExample1.jpg}
		\caption{}
		\label{fig:hogExample1}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{0.17\textwidth}
		\includegraphics[width=\textwidth]{img/hogExample2.png}
		\caption{}
		\label{fig:hogExample2}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{0.17\textwidth}
		\includegraphics[width=\textwidth]{img/hogExample3.png}
		\caption{}
		\label{fig:hogExample3}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{0.17\textwidth}
		\includegraphics[width=\textwidth]{img/hogExample4.jpg}
		\caption{}
		\label{fig:hogExample4}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{0.17\textwidth}
		\includegraphics[width=\textwidth]{img/hogExample5.png}
		\caption{}
		\label{fig:hogExample5}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{0.17\textwidth}
		\includegraphics[width=\textwidth]{img/hogExample6.png}
		\caption{}
		\label{fig:hogExample6}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{0.17\textwidth}
		\includegraphics[width=\textwidth]{img/hogExample7.png}
		\caption{}
		\label{fig:hogExample7}
		\vspace{2mm}
	\end{subfigure}}
	\caption{Example of the HOG descriptor taken from \cite[Figure 6]{dalal2005histograms}. \subref{fig:hogExample1} shows the average gradient image over training data. \subref{fig:hogExample2} and \subref{fig:hogExample3} show the maximal positive and minimal negative SVM weights for each block centered at the pixels, respectively. \subref{fig:hogExample4} shows a test image. \subref{fig:hogExample5} shows its computed descriptor. \subref{fig:hogExample6} and \subref{fig:hogExample7} show the descriptor weighted by positive and negative SVM weights, respectively.}
	\label{fig:hogExample}
%	\stepcounter{figure}
	\vspace*{\floatsep}
	\setcounter{subfigure}{0}
	\centering
	\centerline{
	\begin{subfigure}[t]{0.1635\textwidth}
		\includegraphics[width=\textwidth]{img/inriaPosTrainMeanM.png}
		\caption{}
		\label{fig:inriaPosTrainMeanM}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{0.17\textwidth}
		\includegraphics[width=\textwidth]{img/inriaExampleDescriptorSvmMax.pdf}
		\caption{}
		\label{fig:inriaExampleDescriptorSvmMax}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{0.17\textwidth}
		\includegraphics[width=\textwidth]{img/inriaExampleDescriptorSvmMin.pdf}
		\caption{}
		\label{fig:inriaExampleDescriptorSvmMin}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{0.17\textwidth}
		\includegraphics[width=\textwidth]{img/inriaExampleCells.pdf}
		\caption{}
		\label{fig:inriaExampleCells}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{0.17\textwidth}
		\includegraphics[width=\textwidth]{img/inriaExampleDescriptor.pdf}
		\caption{}
		\label{fig:inriaExampleDescriptor}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{0.17\textwidth}
		\includegraphics[width=\textwidth]{img/inriaExampleDescriptorSvm.pdf}
		\caption{}
		\label{fig:inriaExampleDescriptorSvm}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{0.17\textwidth}
		\includegraphics[width=\textwidth]{img/inriaExampleDescriptorSvmNeg.pdf}
		\caption{}
		\label{fig:inriaExampleDescriptorSvmNeg}
		\vspace{2mm}
	\end{subfigure}}
	\caption{Example of our GO descriptor following the same procedure as \Cref{fig:hogExample}. \subref{fig:inriaExampleDescriptorSvmMax} and \subref{fig:inriaExampleDescriptorSvmMin} are however shown for each cell instead of the HOG blocks, and upon \subref{fig:inriaExampleCells} we have illustrated out grid layout using the cell spacing $r$.}
	\label{fig:inriaExample}
\end{figure}
%
\begin{figure}[p]
	\begin{subfigure}[t]{\textwidth}
		\centerline{\includegraphics[width=1.2\textwidth]{img/objectDetectionTP.png}}
		\caption{True positives}
		\label{fig:objectDetectionTP}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{\textwidth}
		\centerline{\includegraphics[width=1.2\textwidth]{img/objectDetectionFN.png}}
		\caption{False negatives}
		\label{fig:objectDetectionFN}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{\textwidth}
		\centerline{\includegraphics[width=1.2\textwidth]{img/objectDetectionTN.png}}
		\caption{True negatives}
		\label{fig:objectDetectionTN}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{\textwidth}
		\centerline{\includegraphics[width=1.2\textwidth]{img/objectDetectionFP.png}}
		\caption{False positives}
		\label{fig:objectDetectionFP}
	\end{subfigure}
	\caption{Example classifications of INRIA cut-outs by our GO descriptor. This shows the types of images our trained SVM is best and worst at classifying.}
	\label{fig:ownDescriptorExample}
\end{figure}
%
\Cref{fig:hogExample} shows a descriptor example from \citet{dalal2005histograms}, and \Cref{fig:inriaExample} shows the corresponding images reproduced for our GO descriptor. Since HOG has each histogram of gradients ``repeated'' by using block normalization, only the blocks centered at each pixel are illustrated. Our descriptor is illustrated for each cell of the descriptor. The average gradient images over the training data in \subref{fig:inriaPosTrainMeanM} are similar to one another, as they both are computed using central difference filters without smoothing. Likewise we clearly see the silhouette of a pedestrian in these average images.

The maximal positive and minimal negative SVM weights in \subref{fig:inriaExampleDescriptorSvmMax} and \subref{fig:inriaExampleDescriptorSvmMin} differ quite a lot since our descriptor has a much more dense grid. There are however some overall similarities such as large weights around the head and foot regions of the positive weights, and low weights around the foot regions of the negative SVM weights. In other words structure around the head and foot regions count towards a pedestrian detection.

Having described the SVM weights, we now look at the descriptors in \subref{fig:inriaExampleDescriptor} computed from the images in \subref{fig:inriaExampleCells}. The descriptors weighted by the positive and negative SVM weights are illustrated in \subref{fig:inriaExampleDescriptorSvm} and \subref{fig:inriaExampleDescriptorSvmNeg} respectively. Both HOG and our descriptor visualize the head, shoulders, and foot regions of the pedestrian in \subref{fig:inriaExampleDescriptorSvm}. From \subref{fig:inriaExampleDescriptorSvmNeg} we see that vertical lines inside the pedestrian count against a pedestrian detection for HOG, whereas there is little to conclude for our descriptor. As mentioned in \Cref{sec:INRIApitfalls} repeated boundaries could be learned by the SVM. In the visualized descriptor (bottom row) we easily see the repeated border in the bottom of the image, and a couple of these contribute quite a lot to the positive SVM.

To get an idea of the types of images our trained SVM is best and worst at classifying, we find some of the pedestrian and non-pedestrian images with the highest and lowest classification scores. We call the pedestrian images with high classification scores true positives, as we are more likely to classify them correctly. Whether we do so of course depends on the exact threshold chosen. Similar statements can be said for false positives, true negatives, and false negatives. The images are shown in \Cref{fig:ownDescriptorExample}.

Generally the true positive images show pedestrians in standard up-right poses with the front or back to the camera. The clothing is clearly distinctive from the background, and there are little to no occlusions. Furthermore 6 of the 10 true positives illustrated here have a repeated bottom boundary. The false negatives are quite the opposite: many occlusions, people facing sideways, and some dim lighting. The true negative examples are all sky, with very little gradient structure. The false positives are quite varied; some have structure around the possible foot or head region, but it is not obvious to see how they could be classified as pedestrians.
%
\section{Parameter study}
\label{sec:odParameterStudy}
%
%We wish to optimize the descriptor parameters listed in \Cref{sec:descriptorParameters} with the objective of maximizing average matching PR AUC. We do this separately for GO and SI as they may have different optimal parameters, and simply combine these to create the GO+SI descriptor. As mentioned in the experimental setup, we have split the DTU dataset into six parts to reduce overfitting by cross-validation. For each iteration we optimize parameters on five parts and test the optimized descriptor on the final part. This means we obtain six sets of parameters and a single PR AUC for each image, where the images are evaluated without optimizing across their respective scenes. To save time evaluating parameters, we scale the images to half size and convert them to grayscale. The testing is done on the full scale images converted to opponent colour space as mentioned in \Cref{sec:opponentColourSpace}.
%
%Our actual parameter optimization is problematic because we need to optimize ten parameters at once, and each evaluation is computationally heavy. Our basic strategy is thus to cycle through each parameter and optimize it separately while fixing the other parameters. We do this a number of iterations with the hope of reaching convergence.
%The risk of this strategy is getting stuck in local optima. To avoid this, recall that we have defined the parameters to reduce correlation: e.g. when bin count $n$ is increased, the width of the bins is automatically reduced to compensate. We also select certain correlated parameters to optimize together: grid type with grid size, as well as cell and bin kernels with their respective scales. Furthermore we optimize continuous parameters in two steps: first a coarse search and then a finer search around the best result.
%
%The optimal parameters of our parameter study for our GO and SI descriptors are shown in \Cref{fig:ICparamsGo,fig:ICparamsSi} respectively.

We follow the same setup as our image correspondence parameter study in \Cref{sec:icParameterStudy}, with a couple exceptions. Since the dataset has a pre-defined split into training and test data, we only run the parameter study once (with 6-fold cross validation) on training data and obtain a single set of parameters. The parameters are also slightly different of the region cell layout and the addition of the SVM-weight $C$, which we also wish to optimize for each descriptor.

The optimal parameters for our GO and SI descriptors are shown in \Cref{tbl:INRIAparams}, and the optimal $C$ values for all descriptors are shown in \Cref{tbl:INRIAparamC}. The effect of adjusting the parameters relative to the optimized parameters can be seen for GO in \Cref{fig:inriaParametersGoAuc} and SI in \Cref{fig:inriaParametersSiAuc}. Cell spacing $r$ seems to the most important parameter: Decreasing it improves performance greatly, but the increased number of cells also results in a much higher dimensionality. We decided not to allow $r$ values lower than the current $1.6$, because the current dimensionality is already almost too high to work with. Most of the other parameters have a large range of values that produce good results. Box filters outperform the alternatives as cell kernels, while the choice of bin kernel is not very important.

Due to the high dimensionality we construct a compact version of our GO descriptor based on these training results. We keep the same optimal parameters, except we increase cell spacing $r = 1.94$ and reduce bin count $n = 9$. This reduces the dimensionality below HOG. We will go into further detail in the results section.

\begin{table}[p]
\centering
\begin{tabular}{ l c c }
\toprule
{} & \multicolumn{2}{c}{Methods} \\
Parameter & GO & SI \\ \midrule
Grid type & Square & Square \\
Cell spacing $r$ & $1.6$ & $1.6$ \\
Cell kernel & \textit{Box} & \textit{Box} \\
Cell scale $\alpha$ & $3.3$ & $3.5$ \\
Bin count $n$ & $15$ & $9$ \\
Bin kernel & \textit{Tri} & \textit{Box} \\
Bin scale $\beta$ & $1.0$ & $1.5$ \\
Normalization scale $\eta$ & $6.0$ & $5.0$ \\
\bottomrule
\end{tabular}
\caption{Optimized parameters from our parameter study for GO and SI.}
\label{tbl:INRIAparams}
%
\vspace{5mm}
%
\begin{tabular}{ l c c c c }
\toprule
{} & \multicolumn{4}{c}{Methods} \\
Parameter & GO & SI & GO+SI & HOG \\ \midrule
$\log C$ & $1.3\cdot 10^{-3}$ & $2.5\cdot 10^{-3}$ & $1.6\cdot 10^{-3}$ & $1.3\cdot 10^{-3}$ \\
\bottomrule
\end{tabular}
\caption{Optimized SVM weight C for GO, SI, GO+SI, and HOG. TODO: Add compact GO}
\label{tbl:INRIAparamC}
\end{table}
%
\begin{figure}[p]
\centering
	\centerline{\begin{subfigure}[t]{0.593\textwidth}
		\includegraphics[width=\textwidth]{img/inriaParametersGo_cellSpacing.pdf}
		\caption{Cell spacing $r$}
		\label{fig:inriaParametersGo_cellSpacing}
	\end{subfigure}
	\begin{subfigure}[t]{0.593\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/inriaParametersGo_binCount.pdf}
		\caption{Bin count $n$}
		\label{fig:inriaParametersGo_binCount}
	\end{subfigure}}
	\centerline{\begin{subfigure}[t]{0.593\textwidth}
		\includegraphics[width=\textwidth]{img/inriaParametersGo_cellSigmaAlt.pdf}
		\caption{Cell scale $\alpha$}
		\label{fig:inriaParametersGo_cellSigma}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{0.593\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/inriaParametersGo_binSigmaAlt.pdf}
		\caption{Bin scale $\beta$}
		\label{fig:inriaParametersGo_binSigma}
	\end{subfigure}}
	\centerline{\begin{subfigure}[t]{0.593\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/inriaParametersGo_normSigma.pdf}
		\caption{Normalization scale $\eta$}
		\label{fig:inriaParametersGo_normSigma}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{0.593\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/inriaParametersGo_logC.pdf}
		\caption{SVM weight $C$}
		\label{fig:inriaParametersGo_logC}
		\vspace{2mm}
	\end{subfigure}}
	\caption{Average classification performance on training data for GO when adjusting the parameters. Optimal parameter values are marked with a cross.}
	\label{fig:inriaParametersGoAuc}
	\vspace{1cm}
	\centerline{\begin{subfigure}[t]{0.593\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/inriaParametersGo_cellSpacingDims.pdf}
		\caption{Cell spacing $r$}
		\label{fig:inriaParametersGo_cellSpacingDims}
	\end{subfigure}
	\begin{subfigure}[t]{0.593\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/inriaParametersGo_binCountDims.pdf}
		\caption{Bin count $n$}
		\label{fig:inriaParametersGo_binCountDims}
		\vspace{2mm}
	\end{subfigure}}
	\caption{Dimensionality of GO when adjusting the parameters.}
	\label{fig:inriaParametersGoDims}
\end{figure}
%
\begin{figure}[p]
\centering
	\centerline{\begin{subfigure}[t]{0.593\textwidth}
		\includegraphics[width=\textwidth]{img/inriaParametersSi_cellSpacing.pdf}
		\caption{Cell spacing $r$}
		\label{fig:inriaParametersSi_cellSpacing}
	\end{subfigure}
	\begin{subfigure}[t]{0.593\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/inriaParametersSi_binCount.pdf}
		\caption{Bin count $n$}
		\label{fig:inriaParametersSi_binCount}
	\end{subfigure}}
	\centerline{\begin{subfigure}[t]{0.593\textwidth}
		\includegraphics[width=\textwidth]{img/inriaParametersSi_cellSigmaAlt.pdf}
		\caption{Cell scale $\alpha$}
		\label{fig:inriaParametersSi_cellSigma}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{0.593\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/inriaParametersSi_binSigmaAlt.pdf}
		\caption{Bin scale $\beta$}
		\label{fig:inriaParametersSi_binSigma}
	\end{subfigure}}
	\centerline{\begin{subfigure}[t]{0.593\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/inriaParametersSi_normSigma.pdf}
		\caption{Normalization scale $\eta$}
		\label{fig:inriaParametersSi_normSigma}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{0.593\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/inriaParametersSi_logC.pdf}
		\caption{SVM weight $C$}
		\label{fig:inriaParametersSi_logC}
		\vspace{2mm}
	\end{subfigure}}
	\caption{Average classification performance on training data for SI when adjusting the parameters. Optimal parameter values are marked with a cross.}
	\label{fig:inriaParametersSiAuc}
	\vspace{1cm}
	\centerline{\begin{subfigure}[t]{0.593\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/inriaParametersSi_cellSpacingDims.pdf}
		\caption{Cell spacing $r$}
		\label{fig:inriaParametersSi_cellSpacingDims}
	\end{subfigure}
	\begin{subfigure}[t]{0.593\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/inriaParametersSi_binCountDims.pdf}
		\caption{Bin count $n$}
		\label{fig:inriaParametersSi_binCountDims}
		\vspace{2mm}
	\end{subfigure}}
	\caption{Dimensionality of SI when adjusting the parameters.}
	\label{fig:inriaParametersSiDims}
\end{figure}
%
We have also experimented with various other parts of our descriptor that are not listed as parameters. We report some of these results for our optimal GO descriptor on training data, which has a mean PR AUC of $0.972$: Normalizing the final descriptors before training the SVMs reduces the AUC by $0.539$. Computing the gradients on RGB colour channels and taking the largest magnitude improves the AUC by $0.003$. Omitting pixel normalization reduces the AUC by $0.066$. Smoothing images at scale $\sigma = 1$ reduces the AUC by $0.147$. These tendencies hold for SI as well.
%
\section{Results}
%
Now that we have computed optimal parameters on the training data, we can evaluate the performance of the descriptors on test data. \Cref{fig:inriaTestResults} shows ROC- and PR-curves of the various descriptors. Both curves show the same ordering of descriptor performances, from best to worst: GO+SI, GO, compact GO, SI, HOG. The long straight line segments, which appear in both graphs, occur because there are many completely blank negative cut-outs that are all given the same classification score.

\Cref{fig:inriaHistogram} illustrates the distribution of classification scores for positive and negative images. This gives a more intuitive understanding of the performance of the descriptors. We see for instance that GO+SI achieves a much better separation of the two classes than HOG.

\Cref{tbl:inriaResults} shows a performance summary for the five descriptors both with and without hard training. The first thing we notice is the large variation in dimensionality.
%


Notes: Ordering the same on all perf. measures.
Dimensions: Compact GO lower than HOG. GO+SI probably too big for most purposes.
Hard training: Many hard negatives on HOG compared to our descriptors. Improved performance greatly for HOG. Generally a substantial improvement by using hard negatives. Initially 40 windows pr. negative ~ 48k windows. 1024 not many compared to this number, but great improvement.

Generally: Compact GO better than SI.

\begin{table}[tb]
\centerline{
\begin{tabular}{ l c c c c c }
\toprule
Measure & GO+SI & GO & Compact GO & SI & HOG \\ \midrule
Dimensions & 17784 & 11115 & 4185 & 6669 & 4743 \\
Hard negatives & 1024 & 1880 & 4525 & 4135 & 26368 \\
Initial PR AUC & 0.9913 & 0.9881 & 0.9803 & 0.9582 & 0.9027 \\
PR AUC & 0.9943 & 0.9919 & 0.9873 & 0.9656 & 0.9227 \\
ROC AUC & 0.999981 & 0.999979 & 0.999959 & 0.99975 & 0.998375 \\
Recall at $10^{-4}$ FPR & 0.989 & 0.984 & 0.966 & 0.919 & 0.859 \\
\bottomrule
\end{tabular}}
\caption{Summary of test results on the INRIA dataset. The descriptors are ordered from best to worst. For each descriptor we report the dimensionality, number of hard negatives added to the training data, test PR AUC without added hard negatives, and test PR AUC, ROC AUC, and recall at $10^{-4}$ FPR after the hard negatives are added. The latter measure is the one used by \Citet{dalal2005histograms}.}
\label{tbl:inriaResults}
\end{table}
%
\begin{figure}[tb]
\centering
	\centerline{\includegraphics[width=1.2\textwidth]{img/inriaTestResultsLegend.pdf}}
	\centerline{\begin{subfigure}[t]{0.593\textwidth}
		\includegraphics[width=\textwidth]{img/inriaTestResultsROC.pdf}
		\caption{Log-log ROC-curve}
		\label{fig:inriaTestResultsROC}
	\end{subfigure}
	\begin{subfigure}[t]{0.593\textwidth}
		\includegraphics[width=\textwidth]{img/inriaTestResultsPR.pdf}
		\caption{PR-curve}
		\label{fig:inriaTestResultsPR}
	\end{subfigure}}
	\caption{INRIA test results.}
	\label{fig:inriaTestResults}
\end{figure}
%
\begin{figure}
	\centering
	\begin{subfigure}[t]{\textwidth}
		\includegraphics[width=\textwidth]{img/inriaHistogramGoSiFinal.pdf}
		\caption{GO+SI}
		\label{fig:inriaHistogramGoSiFinal}
	\end{subfigure}
	\begin{subfigure}[t]{\textwidth}
		\includegraphics[width=\textwidth]{img/inriaHistogramGoFinal.pdf}
		\caption{GO}
		\label{fig:inriaHistogramGoFinal}
	\end{subfigure}
	\begin{subfigure}[t]{\textwidth}
		\includegraphics[width=\textwidth]{img/inriaHistogramGoChosenSmall.pdf}
		\caption{Compact GO}
		\label{fig:inriaHistogramGoChosenSmall}
	\end{subfigure}
	\begin{subfigure}[t]{\textwidth}
		\includegraphics[width=\textwidth]{img/inriaHistogramSiFinal.pdf}
		\caption{SI}
		\label{fig:inriaHistogramSiFinal}
	\end{subfigure}
	\begin{subfigure}[t]{\textwidth}
		\includegraphics[width=\textwidth]{img/inriaHistogramHog.pdf}
		\caption{HOG}
		\label{fig:inriaHistogramHog}
	\end{subfigure}
	\caption{Histograms of classification scores on INRIA test data for various descriptors. The negative and positive results are individually normalized due to the large skew in amount of images.}
	\label{fig:inriaHistogram}
\end{figure}
%
\subbibliography

\end{document}
