\newcommand{\x}{\mathbf{x}}
\newcommand{\w}{\mathbf{w}}
\documentclass[thesis.tex]{subfiles}
\begin{document}

\chapter{Pedestrian detection}
\label{sec:od}
In this chapter we describe the pedestrian detection problem and how we apply our descriptor to solve it.

Object detection is the problem of locating specified objects in images. One application of this is pedestrian detection, which we have decided to focus on as it is simple and has extensive datasets. A common approach to object detection is to extract regions from the image, reducing the problem to binary classification of the presence of the specified object in each region. Using this approach, we are able to compute one descriptor for each region. We furthermore assume fixed size regions in scale-space, allowing us to directly compare our descriptors. The classification of pedestrians is done using a linear SVM.

The chapter is structured as follows...

\section{Support vector machine}

Suppose we have some training data split into positive and negative elements. In our case these are $d$-dimensional descriptors of regions with and without pedestrians. We wish to construct a binary classifier which accurately assigns a classification score denoting how confident we are that there is or isn't a pedestrian present. The approach of a linear support vector machine (SVM) is to place a $(d-1)$-dimensional hyperplane in descriptor space which separates the two classes. The hyperplane $(\w,b)$ is defined as the points $\x$ satisfying
%
\begin{align*}
\w \cdot \x - b = 0.
\end{align*}
%
If the data is linearly separable, we can search for the \emph{maximum-margin} hyperplane defined as having the largest possible distance to the nearest point of each class. This distance defines the margin. Formally, this can be written as the constrained optimization problem
%
\begin{align*}
\min_{\w,b} \frac12 \| \w \|^2 \qquad \text{subject to} \qquad y_i (\w \cdot \x_i - b) \geq 1,
\end{align*}
%
where $\x_i$ are training data points and $y_i \in \{-1,1\}$ their respective condition labels.

However, linear separation of the two classes is almost never feasible. Instead a \emph{soft-margin} SVM is used. The idea is to allow some of the data points to be placed on the wrong side of the margin. The total distance of misplaced points to the margin is minimized, in addition to maximizing the margin. This is done by introducing a \emph{cost} parameter $C$ which weights the importance of the misplacement minimization relative to the margin maximization. Formally, the soft-margin optimization problem is written as
%
\begin{align*}
\min_{\w,\boldsymbol{\xi},b} \left\{ \frac12 \| \w \|^2 + C \sum_i \xi_i \right\} \qquad \text{subject to} \qquad \begin{aligned} y_i (\w \cdot \x_i - b) &\geq 1 - \xi_i, \\ \xi_i &\geq 0, \end{aligned}
\end{align*}
%
where $\xi_i$ are slack variables allowing for misplaced points.

If there is a large skew in the number of elements in each class, the larger class will have a higher propability of encapsulating a large amount of the variations in the smaller class. If the smaller class is of great importance, the cost parameter $C$ can be defined for each of the two classes individually allowing for a correction for the element skew.

When using the SVM for solving the pedestrian detection problem, we use the $L_2$-regularized $L_2$ loss support vector classifier from LIBLINEAR v. 1.94\footnote{\url{http://www.csie.ntu.edu.tw/~cjlin/liblinear/}} \cite{fan2008liblinear} and hence the $L_2$-norm is used for computing both $\| \w \|$ and $\xi_i$.

\section{Performance measures}

Having described the method of using a SVM for (binary) classification, we are now able to derive a performance measures for the pedestrian detection problem. By using the SVM classifier, we get the signed distance $\w \cdot \x_i - b$ between each descriptor $\x_i$ and the hyperplane dividing the two classes. Using this distance as the classification score $s$ and following the binary classification method described in \Cref{sec:binaryClassificationMeasures}, we are able to compute both the PR- and ROC-curves and their AUCs. In other words we vary a threshold on the distance to the hyperplane to get the two measures of the performance of a descriptor.

When finding pedestrians, only the exact windows from (possibly larger) images with pedestrians of the correct height relative to the window height are positives. The amount of variation in pedestrians is however significantly smaller than the amount of variation in all non-pedestrian images, and hence we will most likely have a large skew in the amount of positives and negatives.
Recall that the PR measure is used when a large skew towards one of the classes of a binary classification is present as described in \Cref{sec:binaryClassificationMeasures}. We will therefore be using the PR measure as performance measure for our pedestrian detection parameter study as well as testing. \Citet{dalal2005histograms} use the log-log plot of the ROC-curve as performance measure, and hence we will show test results for this performance measure as well. In order for this measure to make sense, the actual plot is defined as the logarithm of $1-\text{Recall}$ versus the logarithm of $FPR$.

\section{Dataset}
\label{sec:odDataset}

The dataset we use for training and testing our descriptor on the pedestrian detection application is called the \emph{INRIA Person Dataset}\footnote{\url{http://pascal.inrialpes.fr/data/human/}} (from now on called the INRIA dataset) constructed by \citet{dalal2005histograms}.

It consists of various real world images grouped into two subsets: Images with pedestrians (positives) and images without pedestrians (negatives). The positive images are rescaled cutouts centered around each pedestrian in larger images. The cutouts are first extracted and then individually re-scaled to make the pedestrian of each cutout 96 pixels in height from their feet to shoulders. The size of the cutouts are $64 \times 128$ pixels with a 3 pixel border to avoid boundary condition effects. In case a pedestrian cutout is smaller than the defined size after resizing, the borders are replicated to achieve the desired dimensions.
The positive set only contains images of somewhat upright persons that initially were at least 100 pixels in height. In order to improve the robustness of the dataset against reflected images, each positive cutout has its horizontally flipped image included as well.

The training dataset has a total of 2416 positives and 1218 negative images.
The test dataset has a total of 1126 positives and 453 negative images.

\Cref{fig:inriaExampleImages} shows examples of positive \subref{fig:inriaPositives} and negative \subref{fig:inriaNegatives} images from the INRIA dataset. From the positive examples we clearly see the high variety of upright positions and surroundings in the dataset.

\newgeometry{left=2cm,right=2cm,top=3cm,bottom=3cm}
\begin{figure}[p]
	\centering
	\begin{subfigure}[t]{\textwidth}
		\includegraphics[width=\textwidth]{img/inriaPositives.png}
		\caption{Positives}
		\label{fig:inriaPositives}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{\textwidth}
		\includegraphics[width=\textwidth]{img/inriaNegatives.png}
		\caption{Negatives}
		\label{fig:inriaNegatives}
	\end{subfigure}
	\caption{Example INRIA images.}
	\label{fig:inriaExampleImages}
	%
	\vspace{1cm}
	%
	\begin{subfigure}[t]{0.495\textwidth}
		\includegraphics[width=\textwidth]{img/inriaManExample1.png}
	\end{subfigure}
	\begin{subfigure}[t]{0.495\textwidth}
		\includegraphics[width=\textwidth]{img/inriaManOccluded.png}
	\end{subfigure}
	\caption{Examples of pedestrians in negative images.}
	\label{fig:inriaNegativePersons}
\end{figure}
\restoregeometry

\subsection{Pitfalls and deficiencies}
We have found two smaller problems with the INRIA dataset which could affect the results of using the dataset. The first problem is duplicate images. Amongst the negative training images we have found 10 pair-wise duplicates. This chould bias the SVM since it effectively means that these images are weighted twice as high as the others. Given the small percentage of duplicates in the dataset the effect should however be close to nothing. In the initial training there are furthermore extracted windows at different random positions of the duplicates which decreases the bias even more. We have likewise found one duplicated image in the negative test set.

The second problem is the presence of people in some of the negatives.
\Cref{fig:inriaNegativePersons} show two examples of persons in the negative images of the INRIA dataset. The people are however either too small to fit correctly into the sliding window at any of the used scales or partially occluded by the image bounds to such an extent that we shouldn't be able to detect them anyway.

\section{Experimental setup}
%
We have an two experimental setups for evaluating descriptors on the INRIA dataset. The first is a validation setup for evaluating parameters during our parameter study using only the training data, and the second is a more involved test setup for evaluating the optimized or alternative descriptors on test data.

For our validation setup, we split the training data into six parts for the purpose of (leave-one-out) cross-validation. This is done by, for each split, training an SVM on five parts and testing on the other. The SVM is trained using all the given positive images and 40 random cut-outs from each negative image. The results are consolidated by computing the mean PR AUC across the six splits.

Our test setup is a modification of the one described in \citet{dalal2005histograms}. First, an SVM is trained using all positive training images and 40 random cut-outs from each negative training image. Then a sliding window is run across these negative training images, and these windows are classified by the SVM in order to find false positives. In other words, we search the negative training images for examples not yet learned by the SVM from the initial random cut-outs. These images are called \emph{hard negatives}. We pick a fixed number of hard negatives by taking the $10^5$ largest 


Initial training: X randomly chosen windows per negative image and all positives
Hard training: Predict on sliding window of negatives ~ 2.2 million windows. Add hard negatives to training set and re-train.

Sliding window: 10 pixel in between windows. Multiscale.

Test on test set with sliding window on negatives ~ 1 million windows.

\section{Example}
\newgeometry{left=2cm,right=2cm,top=3cm,bottom=3cm}
\begin{figure}
	\centering
	\begin{subfigure}[t]{0.135\textwidth}
		\includegraphics[width=\textwidth]{img/hogExample1.jpg}
		\caption{}
		\label{fig:hogExample1}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{0.135\textwidth}
		\includegraphics[width=\textwidth]{img/hogExample2.png}
		\caption{}
		\label{fig:hogExample2}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{0.135\textwidth}
		\includegraphics[width=\textwidth]{img/hogExample3.png}
		\caption{}
		\label{fig:hogExample3}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{0.135\textwidth}
		\includegraphics[width=\textwidth]{img/hogExample4.jpg}
		\caption{}
		\label{fig:hogExample4}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{0.135\textwidth}
		\includegraphics[width=\textwidth]{img/hogExample5.png}
		\caption{}
		\label{fig:hogExample5}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{0.135\textwidth}
		\includegraphics[width=\textwidth]{img/hogExample6.png}
		\caption{}
		\label{fig:hogExample6}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{0.135\textwidth}
		\includegraphics[width=\textwidth]{img/hogExample7.png}
		\caption{}
		\label{fig:hogExample7}
		\vspace{2mm}
	\end{subfigure}
	%
	\setcounter{subfigure}{0}
	%
	\begin{subfigure}[t]{0.13\textwidth}
		\includegraphics[width=\textwidth]{img/inriaPosTrainMeanM.png}
		\caption{}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{0.135\textwidth}
		\includegraphics[width=\textwidth]{img/inriaExampleDescriptorSvmMax.pdf}
		\caption{}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{0.135\textwidth}
		\includegraphics[width=\textwidth]{img/inriaExampleDescriptorSvmMin.pdf}
		\caption{}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{0.135\textwidth}
		\includegraphics[width=\textwidth]{img/inriaExampleCells.pdf}
		\caption{}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{0.135\textwidth}
		\includegraphics[width=\textwidth]{img/inriaExampleDescriptor.pdf}
		\caption{}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{0.135\textwidth}
		\includegraphics[width=\textwidth]{img/inriaExampleDescriptorSvm.pdf}
		\caption{}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{0.135\textwidth}
		\includegraphics[width=\textwidth]{img/inriaExampleDescriptorSvmNeg.pdf}
		\caption{}
		\label{fig:inriaExampleDescriptorSvm}
		\vspace{2mm}
	\end{subfigure}
	\caption{Figure taken from HOG \citep{dalal2005histograms} (top) reproduced for our GO descriptor (bottom). \subref{fig:hogExample1} shows the average gradient image over training data. \subref{fig:hogExample2} and \subref{fig:hogExample3} show the maximal positive and minimal negative SVM weights for each cell, respectively. \subref{fig:hogExample4} shows a test image, where we have overlaid our grid layout. \subref{fig:hogExample5} shows its computed descriptor. \subref{fig:hogExample6} and \subref{fig:hogExample7} show the descriptor weighted by positive and negative SVM weights, respectively.}
	\label{fig:imageCorrespondenceCurves}
\end{figure}
\restoregeometry

\newgeometry{left=2cm,right=2cm,top=3cm,bottom=3cm}
\begin{figure}
	\centering
	\begin{subfigure}[t]{\textwidth}
		\includegraphics[width=\textwidth]{img/objectDetectionTP.png}
		\caption{True positives}
		\label{fig:objectDetectionTP}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{\textwidth}
		\includegraphics[width=\textwidth]{img/objectDetectionTN.png}
		\caption{True negatives}
		\label{fig:objectDetectionTN}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{\textwidth}
		\includegraphics[width=\textwidth]{img/objectDetectionFN.png}
		\caption{False negatives}
		\label{fig:objectDetectionFN}
		\vspace{2mm}
	\end{subfigure}
	\begin{subfigure}[t]{\textwidth}
		\includegraphics[width=\textwidth]{img/objectDetectionFP.png}
		\caption{False positives}
		\label{fig:objectDetectionFP}
	\end{subfigure}
	\caption{Example classifications of INRIA windows.}
	\label{fig:imageCorrespondenceCurves}
\end{figure}
\restoregeometry

\section{Parameter study}
\label{sec:odParameterStudy}

\subbibliography

\end{document}
